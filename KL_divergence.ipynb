{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXBgVpITTVa3DFmo2HYJFx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihed01/Churn-Prediction/blob/main/KL_divergence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To implement the univariate mixture of Gaussians as shown in the example from the f-GAN paper, and to reproduce the results, you would need to follow these key steps:\n",
        "\n",
        "1. **Set up the experiment**:\n",
        "   - You're aiming to approximate a mixture of Gaussians by learning a Gaussian distribution. You'll need to represent your model $ Q_{\\theta} $ using a linear function which takes a random $ z $ from a standard normal distribution $ N(0, 1) $\\) and outputs $ G_{\\theta}(z) = \\mu + \\sigma z $, where $ \\theta = (\\mu, \\sigma) $ are the parameters to be learned.\n",
        "\n",
        "2. **Variational function**:\n",
        "   - Use a neural network with two hidden layers having 64 units each and tanh activations for the variational function $ T_{\\omega} $.\n",
        "\n",
        "3. **Optimization**:\n",
        "   - Optimize the objective $ F(\\omega, \\theta) $ using a single-step gradient method. Use mini-batch sampling and an appropriate learning rate (e.g., $ \\eta = 0.01 $) for updating both $ \\omega $ and $ \\theta $.\n",
        "\n",
        "4. **Training**:\n",
        "   - Train $ T_{\\omega} $ and $ Q_{\\theta} $ using various divergence measures like KL, reverse KL, JS, Jeffrey, and Pearson.\n",
        "   - Sample batches of size 1024 for both $ p(x)$ and $ p(z) $.\n",
        "\n",
        "5. **Comparison and evaluation**:\n",
        "   - Compare the learned parameters with the best fit obtained by direct optimization of $ D_{f}(P||Q_{\\theta}) $ with respect to $ \\theta $, which is feasible by solving the required integrals numerically.\n",
        "   - Use $ (\\hat{\\omega}, \\hat{\\theta}) $ to denote the parameters learned through this process, and $ \\theta^{*} $ for the best fit.\n",
        "\n",
        "6. **Plotting**:\n",
        "   - You would need to plot the learned distribution against the true distribution to evaluate how well your generative model is performing.\n",
        "\n",
        "7. **Practical considerations**:\n",
        "   - Use gradient clipping and optimizers like Adam for stability and efficiency.\n",
        "   - Monitor the training closely for convergence and adjust hyperparameters as needed.\n",
        "\n",
        "The results section of the paper suggests that you can expect to see a good correspondence between the gap in objectives and the difference between the fitted means and standard deviations, as well as that the trained model performs best on the divergence it was trained with.\n",
        "\n",
        "This is a high-level summary. You should ensure to adjust the architecture and training specifics to match your setup and computational resources. Additionally, you might need to perform hyperparameter tuning to achieve the best results."
      ],
      "metadata": {
        "id": "l__TpVXmaNhc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import ExponentialLR"
      ],
      "metadata": {
        "id": "_EZS5y_0b2Xr"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialiser les paramètres du modèle et de la fonction variationnelle :"
      ],
      "metadata": {
        "id": "c-gra_6pi0DC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisation He pour les poids\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.kaiming_normal_(m.weight)\n",
        "\n",
        "class GenerativeModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GenerativeModel, self).__init__()\n",
        "        self.mu = nn.Parameter(torch.randn(1) * 0.1)\n",
        "        self.log_sigma = nn.Parameter(torch.randn(1) * 0.1)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.mu + torch.exp(self.log_sigma) * z\n",
        "\n",
        "class VariationalFunction(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(VariationalFunction, self).__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(1, 128),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.LeakyReLU(0.01),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "        self.network.apply(init_weights)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x).squeeze(1)\n",
        "\n",
        "def sample_mixture_gaussians(batch_size, w, m1, v1, m2, v2):\n",
        "    n_samples_first = int(batch_size * w)\n",
        "    n_samples_second = batch_size - n_samples_first\n",
        "    samples_first = torch.randn(n_samples_first, 1) * torch.sqrt(torch.tensor(v1)) + m1\n",
        "    samples_second = torch.randn(n_samples_second, 1) * torch.sqrt(torch.tensor(v2)) + m2\n",
        "    return torch.cat([samples_first, samples_second], dim=0)\n",
        "\n",
        "def kl_divergence(x_real, x_fake, T_omega):\n",
        "    t_real = T_omega(x_real)\n",
        "    t_fake = T_omega(x_fake)\n",
        "    kl_loss = torch.mean(t_real) - torch.mean(torch.exp(t_fake - t_fake.max()))  # stabilisation\n",
        "    return kl_loss\n",
        "\n",
        "# Paramètres du mélange de gaussiennes\n",
        "w, m1, v1, m2, v2 = 0.67, -1, 0.0625, 2, 2\n",
        "\n",
        "# Définition de la grille d'hyperparamètres\n",
        "param_grid = {\n",
        "    'learning_rate': [0.001, 0.01, 0.1],\n",
        "    'batch_size': [512, 1024, 2048],\n",
        "    'num_epochs': [1000, 5000, 10000]\n",
        "}\n",
        "\n",
        "# Création de la grille d'hyperparamètres\n",
        "grid = list(ParameterGrid(param_grid))\n",
        "\n",
        "# Initialisation des variables pour stocker le meilleur modèle\n",
        "best_loss = np.inf\n",
        "best_params = None\n",
        "best_model_state = None\n",
        "\n",
        "# Boucle sur toutes les combinaisons de la grille d'hyperparamètres\n",
        "for params in grid:\n",
        "    print(f\"Training with params: {params}\")\n",
        "\n",
        "    # Initialisation des modèles et des optimiseurs\n",
        "    G = GenerativeModel()\n",
        "    T = VariationalFunction()\n",
        "    optimizer_G = optim.Adam(G.parameters(), lr=params['learning_rate'])\n",
        "    optimizer_T = optim.Adam(T.parameters(), lr=params['learning_rate'])\n",
        "\n",
        "    # Annealing du taux d'apprentissage\n",
        "    scheduler_G = ExponentialLR(optimizer_G, gamma=0.9)\n",
        "    scheduler_T = ExponentialLR(optimizer_T, gamma=0.9)\n",
        "\n",
        "    # Boucle d'entraînement\n",
        "    for epoch in range(params['num_epochs']):\n",
        "        x_real = sample_mixture_gaussians(params['batch_size'], w, m1, v1, m2, v2)\n",
        "        z = torch.randn(params['batch_size'], 1)\n",
        "        x_fake = G(z)\n",
        "\n",
        "        optimizer_T.zero_grad()\n",
        "        loss_T = kl_divergence(x_real, x_fake.detach(), T)\n",
        "        loss_T.backward()\n",
        "        optimizer_T.step()\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "        loss_G = kl_divergence(x_real, x_fake, T)\n",
        "        (-loss_G).backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # Mise à jour du taux d'apprentissage\n",
        "        if epoch % 1000 == 0:\n",
        "            scheduler_G.step()\n",
        "            scheduler_T.step()\n",
        "\n",
        "        if epoch % 100 == 0:\n",
        "            print(f\"Epoch {epoch}: Loss_T = {loss_T.item()}, Loss_G = {loss_G.item()}\")\n",
        "\n",
        "    final_loss = loss_G.item()  # Utilisation de loss_G comme critère d'évaluation\n",
        "\n",
        "    if final_loss < best_loss:\n",
        "        best_loss = final_loss\n",
        "        best_params = params\n",
        "        best_model_state = {\n",
        "            'G_state_dict': G.state_dict(),\n",
        "            'T_state_dict': T.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_T_state_dict': optimizer_T.state_dict(),\n",
        "            'loss': best_loss,\n",
        "            'params': best_params\n",
        "        }\n",
        "        torch.save(best_model_state, 'best_model.pth')\n",
        "\n",
        "print(f\"Best Loss: {best_loss}\")\n",
        "print(f\"Best Hyperparameters: {best_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojMUlkyCIHYu",
        "outputId": "d164efc8-7778-401a-dbae-9a4451ab18e1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training with params: {'batch_size': 512, 'learning_rate': 0.001, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.0766449123620987, Loss_G = 0.02300504595041275\n",
            "Epoch 100: Loss_T = -1.1129741668701172, Loss_G = -1.2127169370651245\n",
            "Epoch 200: Loss_T = -4.586388111114502, Loss_G = -4.50886344909668\n",
            "Epoch 300: Loss_T = -10.7691011428833, Loss_G = -11.137742042541504\n",
            "Epoch 400: Loss_T = -19.737993240356445, Loss_G = -20.00797462463379\n",
            "Epoch 500: Loss_T = -31.7608585357666, Loss_G = -31.75788688659668\n",
            "Epoch 600: Loss_T = -46.416561126708984, Loss_G = -46.42390823364258\n",
            "Epoch 700: Loss_T = -62.44626998901367, Loss_G = -64.31195831298828\n",
            "Epoch 800: Loss_T = -82.93164825439453, Loss_G = -82.40505981445312\n",
            "Epoch 900: Loss_T = -103.87023162841797, Loss_G = -107.03396606445312\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.001, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = -0.022651832550764084, Loss_G = -0.1818065196275711\n",
            "Epoch 100: Loss_T = -1.4270505905151367, Loss_G = -1.491921067237854\n",
            "Epoch 200: Loss_T = -4.755482196807861, Loss_G = -4.814642429351807\n",
            "Epoch 300: Loss_T = -11.07669448852539, Loss_G = -11.211793899536133\n",
            "Epoch 400: Loss_T = -20.15705680847168, Loss_G = -20.256425857543945\n",
            "Epoch 500: Loss_T = -32.27259063720703, Loss_G = -32.40067672729492\n",
            "Epoch 600: Loss_T = -46.61076736450195, Loss_G = -47.447357177734375\n",
            "Epoch 700: Loss_T = -63.80778121948242, Loss_G = -63.9757080078125\n",
            "Epoch 800: Loss_T = -82.0150146484375, Loss_G = -83.47489929199219\n",
            "Epoch 900: Loss_T = -104.99824523925781, Loss_G = -105.71546173095703\n",
            "Epoch 1000: Loss_T = -127.70765686035156, Loss_G = -129.8762664794922\n",
            "Epoch 1100: Loss_T = -152.3950653076172, Loss_G = -151.8551483154297\n",
            "Epoch 1200: Loss_T = -177.61672973632812, Loss_G = -177.327392578125\n",
            "Epoch 1300: Loss_T = -199.61764526367188, Loss_G = -202.69647216796875\n",
            "Epoch 1400: Loss_T = -231.64776611328125, Loss_G = -229.54986572265625\n",
            "Epoch 1500: Loss_T = -261.28106689453125, Loss_G = -260.74090576171875\n",
            "Epoch 1600: Loss_T = -287.88018798828125, Loss_G = -289.2044677734375\n",
            "Epoch 1700: Loss_T = -321.24945068359375, Loss_G = -321.3946838378906\n",
            "Epoch 1800: Loss_T = -351.1213073730469, Loss_G = -357.3117370605469\n",
            "Epoch 1900: Loss_T = -391.6491394042969, Loss_G = -392.4901123046875\n",
            "Epoch 2000: Loss_T = -425.6410217285156, Loss_G = -426.5003662109375\n",
            "Epoch 2100: Loss_T = -459.5942077636719, Loss_G = -456.9090270996094\n",
            "Epoch 2200: Loss_T = -490.8440856933594, Loss_G = -491.5492248535156\n",
            "Epoch 2300: Loss_T = -526.4190673828125, Loss_G = -530.7880859375\n",
            "Epoch 2400: Loss_T = -561.8589477539062, Loss_G = -563.1116943359375\n",
            "Epoch 2500: Loss_T = -601.107666015625, Loss_G = -599.7708740234375\n",
            "Epoch 2600: Loss_T = -634.358642578125, Loss_G = -639.6904907226562\n",
            "Epoch 2700: Loss_T = -670.3146362304688, Loss_G = -673.6317749023438\n",
            "Epoch 2800: Loss_T = -719.3826293945312, Loss_G = -718.744384765625\n",
            "Epoch 2900: Loss_T = -760.43310546875, Loss_G = -755.345703125\n",
            "Epoch 3000: Loss_T = -797.4096069335938, Loss_G = -787.499755859375\n",
            "Epoch 3100: Loss_T = -833.3298950195312, Loss_G = -844.52099609375\n",
            "Epoch 3200: Loss_T = -875.4838256835938, Loss_G = -875.9452514648438\n",
            "Epoch 3300: Loss_T = -914.9742431640625, Loss_G = -901.24169921875\n",
            "Epoch 3400: Loss_T = -945.9959716796875, Loss_G = -954.1038818359375\n",
            "Epoch 3500: Loss_T = -995.4681396484375, Loss_G = -1000.7392578125\n",
            "Epoch 3600: Loss_T = -1038.6904296875, Loss_G = -1024.3974609375\n",
            "Epoch 3700: Loss_T = -1083.9083251953125, Loss_G = -1059.9736328125\n",
            "Epoch 3800: Loss_T = -1121.421875, Loss_G = -1119.3114013671875\n",
            "Epoch 3900: Loss_T = -1166.908935546875, Loss_G = -1157.85107421875\n",
            "Epoch 4000: Loss_T = -1213.499755859375, Loss_G = -1211.809326171875\n",
            "Epoch 4100: Loss_T = -1239.3924560546875, Loss_G = -1240.5714111328125\n",
            "Epoch 4200: Loss_T = -1268.5152587890625, Loss_G = -1281.442138671875\n",
            "Epoch 4300: Loss_T = -1319.1337890625, Loss_G = -1325.18896484375\n",
            "Epoch 4400: Loss_T = -1355.431884765625, Loss_G = -1365.3121337890625\n",
            "Epoch 4500: Loss_T = -1397.4166259765625, Loss_G = -1415.368896484375\n",
            "Epoch 4600: Loss_T = -1455.5284423828125, Loss_G = -1463.134521484375\n",
            "Epoch 4700: Loss_T = -1496.8857421875, Loss_G = -1482.5384521484375\n",
            "Epoch 4800: Loss_T = -1532.4595947265625, Loss_G = -1542.0322265625\n",
            "Epoch 4900: Loss_T = -1584.4906005859375, Loss_G = -1574.6929931640625\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.001, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = 0.13029845058918, Loss_G = 0.18510912358760834\n",
            "Epoch 100: Loss_T = -1.02138352394104, Loss_G = -1.0783960819244385\n",
            "Epoch 200: Loss_T = -4.048785209655762, Loss_G = -4.1669816970825195\n",
            "Epoch 300: Loss_T = -10.139400482177734, Loss_G = -9.974332809448242\n",
            "Epoch 400: Loss_T = -18.84464454650879, Loss_G = -18.942752838134766\n",
            "Epoch 500: Loss_T = -30.4475040435791, Loss_G = -30.324777603149414\n",
            "Epoch 600: Loss_T = -44.65098190307617, Loss_G = -44.62921905517578\n",
            "Epoch 700: Loss_T = -62.45170974731445, Loss_G = -62.24504470825195\n",
            "Epoch 800: Loss_T = -80.5888671875, Loss_G = -81.01934814453125\n",
            "Epoch 900: Loss_T = -102.5992431640625, Loss_G = -102.92794036865234\n",
            "Epoch 1000: Loss_T = -127.85115814208984, Loss_G = -127.92186737060547\n",
            "Epoch 1100: Loss_T = -150.1482696533203, Loss_G = -151.25411987304688\n",
            "Epoch 1200: Loss_T = -174.6538543701172, Loss_G = -176.46742248535156\n",
            "Epoch 1300: Loss_T = -201.324951171875, Loss_G = -200.96041870117188\n",
            "Epoch 1400: Loss_T = -230.9326629638672, Loss_G = -228.84048461914062\n",
            "Epoch 1500: Loss_T = -257.9261779785156, Loss_G = -259.126708984375\n",
            "Epoch 1600: Loss_T = -286.23828125, Loss_G = -287.0024108886719\n",
            "Epoch 1700: Loss_T = -321.65753173828125, Loss_G = -316.6562805175781\n",
            "Epoch 1800: Loss_T = -354.7263488769531, Loss_G = -356.4017333984375\n",
            "Epoch 1900: Loss_T = -387.22509765625, Loss_G = -387.5904541015625\n",
            "Epoch 2000: Loss_T = -423.6597595214844, Loss_G = -423.9175109863281\n",
            "Epoch 2100: Loss_T = -457.6786193847656, Loss_G = -460.1513977050781\n",
            "Epoch 2200: Loss_T = -491.51275634765625, Loss_G = -491.52825927734375\n",
            "Epoch 2300: Loss_T = -524.3123168945312, Loss_G = -522.1798706054688\n",
            "Epoch 2400: Loss_T = -560.4881591796875, Loss_G = -561.580322265625\n",
            "Epoch 2500: Loss_T = -597.1735229492188, Loss_G = -600.9208984375\n",
            "Epoch 2600: Loss_T = -637.7420043945312, Loss_G = -633.2047729492188\n",
            "Epoch 2700: Loss_T = -668.9873657226562, Loss_G = -676.60986328125\n",
            "Epoch 2800: Loss_T = -717.702392578125, Loss_G = -712.6990356445312\n",
            "Epoch 2900: Loss_T = -760.4959106445312, Loss_G = -751.593505859375\n",
            "Epoch 3000: Loss_T = -791.6102294921875, Loss_G = -791.4893188476562\n",
            "Epoch 3100: Loss_T = -829.984130859375, Loss_G = -830.9146728515625\n",
            "Epoch 3200: Loss_T = -867.9178466796875, Loss_G = -869.2635498046875\n",
            "Epoch 3300: Loss_T = -911.7809448242188, Loss_G = -905.41064453125\n",
            "Epoch 3400: Loss_T = -942.8578491210938, Loss_G = -952.2410278320312\n",
            "Epoch 3500: Loss_T = -994.4750366210938, Loss_G = -986.12841796875\n",
            "Epoch 3600: Loss_T = -1033.6357421875, Loss_G = -1021.8278198242188\n",
            "Epoch 3700: Loss_T = -1075.943359375, Loss_G = -1086.140869140625\n",
            "Epoch 3800: Loss_T = -1110.9461669921875, Loss_G = -1112.7947998046875\n",
            "Epoch 3900: Loss_T = -1151.6588134765625, Loss_G = -1159.5699462890625\n",
            "Epoch 4000: Loss_T = -1206.380126953125, Loss_G = -1190.2176513671875\n",
            "Epoch 4100: Loss_T = -1246.9678955078125, Loss_G = -1225.8753662109375\n",
            "Epoch 4200: Loss_T = -1279.9080810546875, Loss_G = -1275.51220703125\n",
            "Epoch 4300: Loss_T = -1310.1165771484375, Loss_G = -1311.7166748046875\n",
            "Epoch 4400: Loss_T = -1361.0120849609375, Loss_G = -1348.31787109375\n",
            "Epoch 4500: Loss_T = -1415.385986328125, Loss_G = -1390.6817626953125\n",
            "Epoch 4600: Loss_T = -1429.28955078125, Loss_G = -1436.5283203125\n",
            "Epoch 4700: Loss_T = -1485.5089111328125, Loss_G = -1487.3594970703125\n",
            "Epoch 4800: Loss_T = -1530.371337890625, Loss_G = -1539.2584228515625\n",
            "Epoch 4900: Loss_T = -1561.178466796875, Loss_G = -1563.2349853515625\n",
            "Epoch 5000: Loss_T = -1600.7462158203125, Loss_G = -1606.405029296875\n",
            "Epoch 5100: Loss_T = -1654.1156005859375, Loss_G = -1640.507568359375\n",
            "Epoch 5200: Loss_T = -1680.4171142578125, Loss_G = -1699.5133056640625\n",
            "Epoch 5300: Loss_T = -1742.9202880859375, Loss_G = -1727.8089599609375\n",
            "Epoch 5400: Loss_T = -1761.8499755859375, Loss_G = -1776.0909423828125\n",
            "Epoch 5500: Loss_T = -1809.488037109375, Loss_G = -1810.28125\n",
            "Epoch 5600: Loss_T = -1859.2108154296875, Loss_G = -1841.512451171875\n",
            "Epoch 5700: Loss_T = -1902.80517578125, Loss_G = -1898.88916015625\n",
            "Epoch 5800: Loss_T = -1916.64794921875, Loss_G = -1930.773681640625\n",
            "Epoch 5900: Loss_T = -1986.0118408203125, Loss_G = -1966.64306640625\n",
            "Epoch 6000: Loss_T = -2010.0933837890625, Loss_G = -2034.4232177734375\n",
            "Epoch 6100: Loss_T = -2050.2451171875, Loss_G = -2055.876953125\n",
            "Epoch 6200: Loss_T = -2094.406494140625, Loss_G = -2111.724853515625\n",
            "Epoch 6300: Loss_T = -2139.775390625, Loss_G = -2129.915283203125\n",
            "Epoch 6400: Loss_T = -2165.30224609375, Loss_G = -2178.487548828125\n",
            "Epoch 6500: Loss_T = -2231.430419921875, Loss_G = -2229.30078125\n",
            "Epoch 6600: Loss_T = -2261.10302734375, Loss_G = -2259.358154296875\n",
            "Epoch 6700: Loss_T = -2300.80322265625, Loss_G = -2270.03662109375\n",
            "Epoch 6800: Loss_T = -2326.62939453125, Loss_G = -2301.406982421875\n",
            "Epoch 6900: Loss_T = -2368.4833984375, Loss_G = -2376.591796875\n",
            "Epoch 7000: Loss_T = -2444.71728515625, Loss_G = -2421.6865234375\n",
            "Epoch 7100: Loss_T = -2429.043212890625, Loss_G = -2455.21044921875\n",
            "Epoch 7200: Loss_T = -2486.18701171875, Loss_G = -2493.41455078125\n",
            "Epoch 7300: Loss_T = -2540.50146484375, Loss_G = -2532.567626953125\n",
            "Epoch 7400: Loss_T = -2549.34619140625, Loss_G = -2549.41064453125\n",
            "Epoch 7500: Loss_T = -2610.81982421875, Loss_G = -2574.344482421875\n",
            "Epoch 7600: Loss_T = -2642.5361328125, Loss_G = -2607.80615234375\n",
            "Epoch 7700: Loss_T = -2691.630859375, Loss_G = -2687.999267578125\n",
            "Epoch 7800: Loss_T = -2722.231689453125, Loss_G = -2724.642822265625\n",
            "Epoch 7900: Loss_T = -2758.60693359375, Loss_G = -2774.84765625\n",
            "Epoch 8000: Loss_T = -2823.265380859375, Loss_G = -2772.9052734375\n",
            "Epoch 8100: Loss_T = -2834.37548828125, Loss_G = -2822.055908203125\n",
            "Epoch 8200: Loss_T = -2886.894775390625, Loss_G = -2883.466552734375\n",
            "Epoch 8300: Loss_T = -2929.48291015625, Loss_G = -2900.596923828125\n",
            "Epoch 8400: Loss_T = -2932.2919921875, Loss_G = -2921.65771484375\n",
            "Epoch 8500: Loss_T = -2992.111572265625, Loss_G = -2972.03076171875\n",
            "Epoch 8600: Loss_T = -3033.3388671875, Loss_G = -3001.651611328125\n",
            "Epoch 8700: Loss_T = -3110.38427734375, Loss_G = -3043.8759765625\n",
            "Epoch 8800: Loss_T = -3096.756103515625, Loss_G = -3112.11376953125\n",
            "Epoch 8900: Loss_T = -3148.4306640625, Loss_G = -3097.53564453125\n",
            "Epoch 9000: Loss_T = -3150.791259765625, Loss_G = -3184.981689453125\n",
            "Epoch 9100: Loss_T = -3207.535888671875, Loss_G = -3190.15478515625\n",
            "Epoch 9200: Loss_T = -3236.463134765625, Loss_G = -3217.867919921875\n",
            "Epoch 9300: Loss_T = -3265.432373046875, Loss_G = -3258.789794921875\n",
            "Epoch 9400: Loss_T = -3303.456787109375, Loss_G = -3309.31982421875\n",
            "Epoch 9500: Loss_T = -3360.4931640625, Loss_G = -3349.94140625\n",
            "Epoch 9600: Loss_T = -3356.1884765625, Loss_G = -3372.5166015625\n",
            "Epoch 9700: Loss_T = -3373.356689453125, Loss_G = -3398.81005859375\n",
            "Epoch 9800: Loss_T = -3414.23876953125, Loss_G = -3500.4990234375\n",
            "Epoch 9900: Loss_T = -3478.652587890625, Loss_G = -3476.292724609375\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.01, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = -0.0026253419928252697, Loss_G = -0.2883574962615967\n",
            "Epoch 100: Loss_T = -101.67578125, Loss_G = -104.13948059082031\n",
            "Epoch 200: Loss_T = -477.8377685546875, Loss_G = -484.9353942871094\n",
            "Epoch 300: Loss_T = -1138.6650390625, Loss_G = -1148.6953125\n",
            "Epoch 400: Loss_T = -2079.1982421875, Loss_G = -2095.1357421875\n",
            "Epoch 500: Loss_T = -3293.8525390625, Loss_G = -3325.431884765625\n",
            "Epoch 600: Loss_T = -4718.228515625, Loss_G = -4797.44580078125\n",
            "Epoch 700: Loss_T = -6496.64453125, Loss_G = -6512.2333984375\n",
            "Epoch 800: Loss_T = -8498.8974609375, Loss_G = -8470.943359375\n",
            "Epoch 900: Loss_T = -10572.65625, Loss_G = -10470.369140625\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.01, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.09201312065124512, Loss_G = -0.06370308250188828\n",
            "Epoch 100: Loss_T = -97.90572357177734, Loss_G = -100.87004852294922\n",
            "Epoch 200: Loss_T = -477.2366943359375, Loss_G = -479.4201354980469\n",
            "Epoch 300: Loss_T = -1129.805908203125, Loss_G = -1135.82275390625\n",
            "Epoch 400: Loss_T = -2075.614990234375, Loss_G = -2099.77783203125\n",
            "Epoch 500: Loss_T = -3297.220947265625, Loss_G = -3287.161865234375\n",
            "Epoch 600: Loss_T = -4769.64111328125, Loss_G = -4778.86328125\n",
            "Epoch 700: Loss_T = -6484.53466796875, Loss_G = -6510.83056640625\n",
            "Epoch 800: Loss_T = -8379.05078125, Loss_G = -8496.1435546875\n",
            "Epoch 900: Loss_T = -10580.9560546875, Loss_G = -10627.724609375\n",
            "Epoch 1000: Loss_T = -13029.63671875, Loss_G = -12913.109375\n",
            "Epoch 1100: Loss_T = -15378.634765625, Loss_G = -15446.6162109375\n",
            "Epoch 1200: Loss_T = -17865.857421875, Loss_G = -17795.6875\n",
            "Epoch 1300: Loss_T = -20404.138671875, Loss_G = -20399.03125\n",
            "Epoch 1400: Loss_T = -22987.162109375, Loss_G = -23065.6328125\n",
            "Epoch 1500: Loss_T = -26074.50390625, Loss_G = -26055.337890625\n",
            "Epoch 1600: Loss_T = -28976.146484375, Loss_G = -29270.845703125\n",
            "Epoch 1700: Loss_T = -32454.048828125, Loss_G = -32293.880859375\n",
            "Epoch 1800: Loss_T = -35427.8359375, Loss_G = -35823.0859375\n",
            "Epoch 1900: Loss_T = -39185.3125, Loss_G = -38998.3828125\n",
            "Epoch 2000: Loss_T = -42749.5546875, Loss_G = -42701.1328125\n",
            "Epoch 2100: Loss_T = -46298.7890625, Loss_G = -45920.9453125\n",
            "Epoch 2200: Loss_T = -48796.8359375, Loss_G = -49751.0625\n",
            "Epoch 2300: Loss_T = -53002.828125, Loss_G = -52968.421875\n",
            "Epoch 2400: Loss_T = -56142.6796875, Loss_G = -56582.390625\n",
            "Epoch 2500: Loss_T = -59680.65625, Loss_G = -60204.734375\n",
            "Epoch 2600: Loss_T = -64000.8671875, Loss_G = -63551.296875\n",
            "Epoch 2700: Loss_T = -67497.0078125, Loss_G = -68675.2265625\n",
            "Epoch 2800: Loss_T = -71473.078125, Loss_G = -72362.0\n",
            "Epoch 2900: Loss_T = -75239.5625, Loss_G = -75615.703125\n",
            "Epoch 3000: Loss_T = -79195.3828125, Loss_G = -79950.390625\n",
            "Epoch 3100: Loss_T = -83632.6640625, Loss_G = -85161.828125\n",
            "Epoch 3200: Loss_T = -86868.265625, Loss_G = -87051.7734375\n",
            "Epoch 3300: Loss_T = -90963.765625, Loss_G = -91309.046875\n",
            "Epoch 3400: Loss_T = -94368.28125, Loss_G = -95347.875\n",
            "Epoch 3500: Loss_T = -100115.578125, Loss_G = -100244.984375\n",
            "Epoch 3600: Loss_T = -101788.4609375, Loss_G = -103114.3984375\n",
            "Epoch 3700: Loss_T = -107985.3046875, Loss_G = -107311.6953125\n",
            "Epoch 3800: Loss_T = -111135.484375, Loss_G = -111504.859375\n",
            "Epoch 3900: Loss_T = -116752.1953125, Loss_G = -116308.3828125\n",
            "Epoch 4000: Loss_T = -119508.203125, Loss_G = -120200.796875\n",
            "Epoch 4100: Loss_T = -124918.78125, Loss_G = -124219.3515625\n",
            "Epoch 4200: Loss_T = -127968.140625, Loss_G = -127768.0390625\n",
            "Epoch 4300: Loss_T = -131331.15625, Loss_G = -133161.8125\n",
            "Epoch 4400: Loss_T = -137043.78125, Loss_G = -135909.46875\n",
            "Epoch 4500: Loss_T = -141028.28125, Loss_G = -139718.265625\n",
            "Epoch 4600: Loss_T = -143463.8125, Loss_G = -145160.453125\n",
            "Epoch 4700: Loss_T = -147622.9375, Loss_G = -148092.59375\n",
            "Epoch 4800: Loss_T = -153322.96875, Loss_G = -152565.421875\n",
            "Epoch 4900: Loss_T = -156867.09375, Loss_G = -157844.734375\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.01, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = -0.020428091287612915, Loss_G = -0.05179242789745331\n",
            "Epoch 100: Loss_T = -98.80619812011719, Loss_G = -101.94300079345703\n",
            "Epoch 200: Loss_T = -471.08685302734375, Loss_G = -478.6012878417969\n",
            "Epoch 300: Loss_T = -1150.4886474609375, Loss_G = -1146.9241943359375\n",
            "Epoch 400: Loss_T = -2094.04541015625, Loss_G = -2108.556396484375\n",
            "Epoch 500: Loss_T = -3254.099853515625, Loss_G = -3294.362060546875\n",
            "Epoch 600: Loss_T = -4777.36865234375, Loss_G = -4776.546875\n",
            "Epoch 700: Loss_T = -6520.09765625, Loss_G = -6482.28955078125\n",
            "Epoch 800: Loss_T = -8479.7109375, Loss_G = -8468.412109375\n",
            "Epoch 900: Loss_T = -10702.80078125, Loss_G = -10766.515625\n",
            "Epoch 1000: Loss_T = -12966.845703125, Loss_G = -12998.2216796875\n",
            "Epoch 1100: Loss_T = -15052.671875, Loss_G = -15342.22265625\n",
            "Epoch 1200: Loss_T = -17648.345703125, Loss_G = -17891.458984375\n",
            "Epoch 1300: Loss_T = -20480.951171875, Loss_G = -20506.029296875\n",
            "Epoch 1400: Loss_T = -23187.26953125, Loss_G = -23241.71875\n",
            "Epoch 1500: Loss_T = -26032.810546875, Loss_G = -26181.78515625\n",
            "Epoch 1600: Loss_T = -28825.15625, Loss_G = -29404.166015625\n",
            "Epoch 1700: Loss_T = -32284.884765625, Loss_G = -32422.390625\n",
            "Epoch 1800: Loss_T = -35796.40625, Loss_G = -35474.8359375\n",
            "Epoch 1900: Loss_T = -39096.5, Loss_G = -38866.96875\n",
            "Epoch 2000: Loss_T = -42759.359375, Loss_G = -42643.90625\n",
            "Epoch 2100: Loss_T = -45459.875, Loss_G = -46128.7421875\n",
            "Epoch 2200: Loss_T = -49111.015625, Loss_G = -49488.4453125\n",
            "Epoch 2300: Loss_T = -52852.1484375, Loss_G = -52689.265625\n",
            "Epoch 2400: Loss_T = -56722.125, Loss_G = -56804.0\n",
            "Epoch 2500: Loss_T = -60783.7890625, Loss_G = -60381.28125\n",
            "Epoch 2600: Loss_T = -63823.796875, Loss_G = -64404.9375\n",
            "Epoch 2700: Loss_T = -67558.0078125, Loss_G = -67600.2265625\n",
            "Epoch 2800: Loss_T = -71767.578125, Loss_G = -72150.640625\n",
            "Epoch 2900: Loss_T = -75586.3828125, Loss_G = -75839.640625\n",
            "Epoch 3000: Loss_T = -79519.09375, Loss_G = -80116.671875\n",
            "Epoch 3100: Loss_T = -83950.875, Loss_G = -83823.359375\n",
            "Epoch 3200: Loss_T = -87384.1015625, Loss_G = -88126.6796875\n",
            "Epoch 3300: Loss_T = -91705.0, Loss_G = -91591.2734375\n",
            "Epoch 3400: Loss_T = -95542.9140625, Loss_G = -96280.453125\n",
            "Epoch 3500: Loss_T = -99421.5234375, Loss_G = -99734.140625\n",
            "Epoch 3600: Loss_T = -103230.1875, Loss_G = -103676.0546875\n",
            "Epoch 3700: Loss_T = -107619.15625, Loss_G = -108419.109375\n",
            "Epoch 3800: Loss_T = -111205.25, Loss_G = -111725.5546875\n",
            "Epoch 3900: Loss_T = -116705.4140625, Loss_G = -114303.515625\n",
            "Epoch 4000: Loss_T = -120003.6328125, Loss_G = -119681.40625\n",
            "Epoch 4100: Loss_T = -122857.359375, Loss_G = -123231.765625\n",
            "Epoch 4200: Loss_T = -127108.984375, Loss_G = -128676.8046875\n",
            "Epoch 4300: Loss_T = -132826.9375, Loss_G = -131630.828125\n",
            "Epoch 4400: Loss_T = -137418.65625, Loss_G = -136221.4375\n",
            "Epoch 4500: Loss_T = -139838.421875, Loss_G = -140930.578125\n",
            "Epoch 4600: Loss_T = -144756.109375, Loss_G = -144602.875\n",
            "Epoch 4700: Loss_T = -148634.546875, Loss_G = -148464.40625\n",
            "Epoch 4800: Loss_T = -153240.3125, Loss_G = -151494.375\n",
            "Epoch 4900: Loss_T = -159412.53125, Loss_G = -158346.625\n",
            "Epoch 5000: Loss_T = -161157.234375, Loss_G = -162991.921875\n",
            "Epoch 5100: Loss_T = -166405.78125, Loss_G = -166505.453125\n",
            "Epoch 5200: Loss_T = -170824.5625, Loss_G = -169132.296875\n",
            "Epoch 5300: Loss_T = -175362.9375, Loss_G = -173777.328125\n",
            "Epoch 5400: Loss_T = -178631.515625, Loss_G = -178769.015625\n",
            "Epoch 5500: Loss_T = -180921.734375, Loss_G = -181419.484375\n",
            "Epoch 5600: Loss_T = -185904.203125, Loss_G = -185875.125\n",
            "Epoch 5700: Loss_T = -190270.890625, Loss_G = -190187.6875\n",
            "Epoch 5800: Loss_T = -195972.640625, Loss_G = -193856.0\n",
            "Epoch 5900: Loss_T = -199046.53125, Loss_G = -196079.53125\n",
            "Epoch 6000: Loss_T = -203008.765625, Loss_G = -202345.859375\n",
            "Epoch 6100: Loss_T = -206362.078125, Loss_G = -206048.34375\n",
            "Epoch 6200: Loss_T = -212318.828125, Loss_G = -211500.40625\n",
            "Epoch 6300: Loss_T = -215900.84375, Loss_G = -215196.390625\n",
            "Epoch 6400: Loss_T = -216037.4375, Loss_G = -217026.6875\n",
            "Epoch 6500: Loss_T = -223303.390625, Loss_G = -222549.375\n",
            "Epoch 6600: Loss_T = -227387.65625, Loss_G = -226034.78125\n",
            "Epoch 6700: Loss_T = -230021.53125, Loss_G = -231390.578125\n",
            "Epoch 6800: Loss_T = -234693.859375, Loss_G = -231891.359375\n",
            "Epoch 6900: Loss_T = -238063.890625, Loss_G = -236856.953125\n",
            "Epoch 7000: Loss_T = -243429.84375, Loss_G = -242426.21875\n",
            "Epoch 7100: Loss_T = -246408.984375, Loss_G = -244388.765625\n",
            "Epoch 7200: Loss_T = -251577.140625, Loss_G = -251427.21875\n",
            "Epoch 7300: Loss_T = -252661.921875, Loss_G = -254166.46875\n",
            "Epoch 7400: Loss_T = -258865.703125, Loss_G = -258706.96875\n",
            "Epoch 7500: Loss_T = -261352.28125, Loss_G = -262769.71875\n",
            "Epoch 7600: Loss_T = -264271.1875, Loss_G = -265742.0\n",
            "Epoch 7700: Loss_T = -267143.34375, Loss_G = -269329.34375\n",
            "Epoch 7800: Loss_T = -271618.5, Loss_G = -273839.125\n",
            "Epoch 7900: Loss_T = -276926.03125, Loss_G = -278538.25\n",
            "Epoch 8000: Loss_T = -280265.53125, Loss_G = -280659.46875\n",
            "Epoch 8100: Loss_T = -282792.0, Loss_G = -283613.0\n",
            "Epoch 8200: Loss_T = -287836.375, Loss_G = -285222.84375\n",
            "Epoch 8300: Loss_T = -291178.90625, Loss_G = -289956.96875\n",
            "Epoch 8400: Loss_T = -295981.65625, Loss_G = -293703.5625\n",
            "Epoch 8500: Loss_T = -296922.28125, Loss_G = -295951.0625\n",
            "Epoch 8600: Loss_T = -301043.0, Loss_G = -301381.0625\n",
            "Epoch 8700: Loss_T = -306355.1875, Loss_G = -305403.0\n",
            "Epoch 8800: Loss_T = -308997.03125, Loss_G = -307215.78125\n",
            "Epoch 8900: Loss_T = -312133.6875, Loss_G = -314875.40625\n",
            "Epoch 9000: Loss_T = -318379.75, Loss_G = -318431.4375\n",
            "Epoch 9100: Loss_T = -318225.5625, Loss_G = -320158.65625\n",
            "Epoch 9200: Loss_T = -323866.0, Loss_G = -323406.8125\n",
            "Epoch 9300: Loss_T = -326221.875, Loss_G = -326712.84375\n",
            "Epoch 9400: Loss_T = -328183.96875, Loss_G = -329141.8125\n",
            "Epoch 9500: Loss_T = -335234.15625, Loss_G = -335628.65625\n",
            "Epoch 9600: Loss_T = -336898.78125, Loss_G = -336704.8125\n",
            "Epoch 9700: Loss_T = -340158.59375, Loss_G = -338431.46875\n",
            "Epoch 9800: Loss_T = -346450.34375, Loss_G = -345887.46875\n",
            "Epoch 9900: Loss_T = -349873.9375, Loss_G = -349064.4375\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.1, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.151045024394989, Loss_G = -0.8719723224639893\n",
            "Epoch 100: Loss_T = -9668.587890625, Loss_G = -9947.6376953125\n",
            "Epoch 200: Loss_T = -47413.640625, Loss_G = -48049.1640625\n",
            "Epoch 300: Loss_T = -114612.53125, Loss_G = -114560.53125\n",
            "Epoch 400: Loss_T = -210370.1875, Loss_G = -209430.421875\n",
            "Epoch 500: Loss_T = -331621.71875, Loss_G = -332244.625\n",
            "Epoch 600: Loss_T = -480294.6875, Loss_G = -480063.5625\n",
            "Epoch 700: Loss_T = -646984.625, Loss_G = -653755.25\n",
            "Epoch 800: Loss_T = -845564.25, Loss_G = -842202.25\n",
            "Epoch 900: Loss_T = -1059796.75, Loss_G = -1062105.875\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.1, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.04456990957260132, Loss_G = -1.0952411890029907\n",
            "Epoch 100: Loss_T = -9841.939453125, Loss_G = -10094.0068359375\n",
            "Epoch 200: Loss_T = -47826.8125, Loss_G = -48066.0\n",
            "Epoch 300: Loss_T = -112696.9375, Loss_G = -114097.296875\n",
            "Epoch 400: Loss_T = -209496.609375, Loss_G = -208935.6875\n",
            "Epoch 500: Loss_T = -328994.90625, Loss_G = -331102.03125\n",
            "Epoch 600: Loss_T = -474948.21875, Loss_G = -477288.0\n",
            "Epoch 700: Loss_T = -643138.25, Loss_G = -646182.625\n",
            "Epoch 800: Loss_T = -835725.125, Loss_G = -853133.75\n",
            "Epoch 900: Loss_T = -1061457.25, Loss_G = -1057144.5\n",
            "Epoch 1000: Loss_T = -1295395.5, Loss_G = -1304850.75\n",
            "Epoch 1100: Loss_T = -1529932.375, Loss_G = -1543252.25\n",
            "Epoch 1200: Loss_T = -1772002.125, Loss_G = -1784109.375\n",
            "Epoch 1300: Loss_T = -2049057.625, Loss_G = -2040895.5\n",
            "Epoch 1400: Loss_T = -2322657.25, Loss_G = -2331897.25\n",
            "Epoch 1500: Loss_T = -2626454.75, Loss_G = -2603923.5\n",
            "Epoch 1600: Loss_T = -2920587.5, Loss_G = -2921422.5\n",
            "Epoch 1700: Loss_T = -3229092.0, Loss_G = -3245404.25\n",
            "Epoch 1800: Loss_T = -3533564.25, Loss_G = -3557936.25\n",
            "Epoch 1900: Loss_T = -3925617.25, Loss_G = -3881259.0\n",
            "Epoch 2000: Loss_T = -4271626.0, Loss_G = -4285785.5\n",
            "Epoch 2100: Loss_T = -4602505.0, Loss_G = -4606282.0\n",
            "Epoch 2200: Loss_T = -4990043.5, Loss_G = -4952121.5\n",
            "Epoch 2300: Loss_T = -5266368.0, Loss_G = -5314854.5\n",
            "Epoch 2400: Loss_T = -5640287.5, Loss_G = -5670927.5\n",
            "Epoch 2500: Loss_T = -6003767.0, Loss_G = -6061472.0\n",
            "Epoch 2600: Loss_T = -6370630.0, Loss_G = -6387830.0\n",
            "Epoch 2700: Loss_T = -6756787.0, Loss_G = -6720631.0\n",
            "Epoch 2800: Loss_T = -7135693.0, Loss_G = -7189645.5\n",
            "Epoch 2900: Loss_T = -7581925.0, Loss_G = -7548193.0\n",
            "Epoch 3000: Loss_T = -7935485.0, Loss_G = -7932663.5\n",
            "Epoch 3100: Loss_T = -8434150.0, Loss_G = -8461388.0\n",
            "Epoch 3200: Loss_T = -8770992.0, Loss_G = -8777796.0\n",
            "Epoch 3300: Loss_T = -9106781.0, Loss_G = -9163367.0\n",
            "Epoch 3400: Loss_T = -9547822.0, Loss_G = -9498672.0\n",
            "Epoch 3500: Loss_T = -9836267.0, Loss_G = -9965690.0\n",
            "Epoch 3600: Loss_T = -10369728.0, Loss_G = -10370408.0\n",
            "Epoch 3700: Loss_T = -10816131.0, Loss_G = -10665498.0\n",
            "Epoch 3800: Loss_T = -11135762.0, Loss_G = -11069395.0\n",
            "Epoch 3900: Loss_T = -11617357.0, Loss_G = -11646026.0\n",
            "Epoch 4000: Loss_T = -12034903.0, Loss_G = -12124203.0\n",
            "Epoch 4100: Loss_T = -12490863.0, Loss_G = -12366821.0\n",
            "Epoch 4200: Loss_T = -12863097.0, Loss_G = -12893786.0\n",
            "Epoch 4300: Loss_T = -13273857.0, Loss_G = -13174441.0\n",
            "Epoch 4400: Loss_T = -13658864.0, Loss_G = -13600862.0\n",
            "Epoch 4500: Loss_T = -14090123.0, Loss_G = -13970024.0\n",
            "Epoch 4600: Loss_T = -14388322.0, Loss_G = -14332382.0\n",
            "Epoch 4700: Loss_T = -14887430.0, Loss_G = -14875713.0\n",
            "Epoch 4800: Loss_T = -15186452.0, Loss_G = -15418911.0\n",
            "Epoch 4900: Loss_T = -15845537.0, Loss_G = -15738865.0\n",
            "Training with params: {'batch_size': 512, 'learning_rate': 0.1, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = 0.051668260246515274, Loss_G = -1.001699447631836\n",
            "Epoch 100: Loss_T = -9652.498046875, Loss_G = -9952.248046875\n",
            "Epoch 200: Loss_T = -47865.7734375, Loss_G = -47833.53125\n",
            "Epoch 300: Loss_T = -114298.9296875, Loss_G = -114803.0234375\n",
            "Epoch 400: Loss_T = -207348.84375, Loss_G = -208552.265625\n",
            "Epoch 500: Loss_T = -330377.78125, Loss_G = -330570.71875\n",
            "Epoch 600: Loss_T = -474790.0, Loss_G = -473425.4375\n",
            "Epoch 700: Loss_T = -648678.1875, Loss_G = -658726.875\n",
            "Epoch 800: Loss_T = -849801.6875, Loss_G = -838264.625\n",
            "Epoch 900: Loss_T = -1064633.5, Loss_G = -1065220.0\n",
            "Epoch 1000: Loss_T = -1300984.0, Loss_G = -1308853.125\n",
            "Epoch 1100: Loss_T = -1531275.75, Loss_G = -1533372.125\n",
            "Epoch 1200: Loss_T = -1784924.125, Loss_G = -1776643.75\n",
            "Epoch 1300: Loss_T = -2043580.25, Loss_G = -2051033.75\n",
            "Epoch 1400: Loss_T = -2305420.25, Loss_G = -2316328.5\n",
            "Epoch 1500: Loss_T = -2610972.75, Loss_G = -2589996.25\n",
            "Epoch 1600: Loss_T = -2902760.25, Loss_G = -2912488.25\n",
            "Epoch 1700: Loss_T = -3261975.25, Loss_G = -3225968.75\n",
            "Epoch 1800: Loss_T = -3569979.0, Loss_G = -3566183.0\n",
            "Epoch 1900: Loss_T = -3904643.5, Loss_G = -3866013.5\n",
            "Epoch 2000: Loss_T = -4245094.0, Loss_G = -4269689.5\n",
            "Epoch 2100: Loss_T = -4603973.0, Loss_G = -4586359.5\n",
            "Epoch 2200: Loss_T = -4926168.0, Loss_G = -4918144.0\n",
            "Epoch 2300: Loss_T = -5261374.5, Loss_G = -5242229.0\n",
            "Epoch 2400: Loss_T = -5678275.0, Loss_G = -5668662.5\n",
            "Epoch 2500: Loss_T = -5981777.5, Loss_G = -6041776.5\n",
            "Epoch 2600: Loss_T = -6350539.5, Loss_G = -6396016.5\n",
            "Epoch 2700: Loss_T = -6786905.0, Loss_G = -6774031.5\n",
            "Epoch 2800: Loss_T = -7206451.0, Loss_G = -7224981.0\n",
            "Epoch 2900: Loss_T = -7583115.0, Loss_G = -7591749.5\n",
            "Epoch 3000: Loss_T = -8070652.5, Loss_G = -8003453.5\n",
            "Epoch 3100: Loss_T = -8335983.0, Loss_G = -8370745.0\n",
            "Epoch 3200: Loss_T = -8702973.0, Loss_G = -8658698.0\n",
            "Epoch 3300: Loss_T = -9160837.0, Loss_G = -9167156.0\n",
            "Epoch 3400: Loss_T = -9613803.0, Loss_G = -9580964.0\n",
            "Epoch 3500: Loss_T = -10006888.0, Loss_G = -9965462.0\n",
            "Epoch 3600: Loss_T = -10340439.0, Loss_G = -10305338.0\n",
            "Epoch 3700: Loss_T = -10831073.0, Loss_G = -10791821.0\n",
            "Epoch 3800: Loss_T = -11113336.0, Loss_G = -11200394.0\n",
            "Epoch 3900: Loss_T = -11510002.0, Loss_G = -11662093.0\n",
            "Epoch 4000: Loss_T = -11946187.0, Loss_G = -11945859.0\n",
            "Epoch 4100: Loss_T = -12402649.0, Loss_G = -12364486.0\n",
            "Epoch 4200: Loss_T = -12780813.0, Loss_G = -12827376.0\n",
            "Epoch 4300: Loss_T = -13305195.0, Loss_G = -13261922.0\n",
            "Epoch 4400: Loss_T = -13641922.0, Loss_G = -13546178.0\n",
            "Epoch 4500: Loss_T = -14105984.0, Loss_G = -13877398.0\n",
            "Epoch 4600: Loss_T = -14357348.0, Loss_G = -14472530.0\n",
            "Epoch 4700: Loss_T = -14763805.0, Loss_G = -14875265.0\n",
            "Epoch 4800: Loss_T = -15190813.0, Loss_G = -15349144.0\n",
            "Epoch 4900: Loss_T = -15752635.0, Loss_G = -15758006.0\n",
            "Epoch 5000: Loss_T = -16052682.0, Loss_G = -16101232.0\n",
            "Epoch 5100: Loss_T = -16679516.0, Loss_G = -16571471.0\n",
            "Epoch 5200: Loss_T = -17057740.0, Loss_G = -17001708.0\n",
            "Epoch 5300: Loss_T = -17293158.0, Loss_G = -17406330.0\n",
            "Epoch 5400: Loss_T = -17807330.0, Loss_G = -17787856.0\n",
            "Epoch 5500: Loss_T = -18148186.0, Loss_G = -18170988.0\n",
            "Epoch 5600: Loss_T = -18534156.0, Loss_G = -18717962.0\n",
            "Epoch 5700: Loss_T = -18970360.0, Loss_G = -18792984.0\n",
            "Epoch 5800: Loss_T = -19368658.0, Loss_G = -19406848.0\n",
            "Epoch 5900: Loss_T = -19955680.0, Loss_G = -19716872.0\n",
            "Epoch 6000: Loss_T = -20301808.0, Loss_G = -20211896.0\n",
            "Epoch 6100: Loss_T = -20453906.0, Loss_G = -20442166.0\n",
            "Epoch 6200: Loss_T = -20957652.0, Loss_G = -21092254.0\n",
            "Epoch 6300: Loss_T = -21375740.0, Loss_G = -21358270.0\n",
            "Epoch 6400: Loss_T = -21750978.0, Loss_G = -22044998.0\n",
            "Epoch 6500: Loss_T = -22211832.0, Loss_G = -22139452.0\n",
            "Epoch 6600: Loss_T = -22713238.0, Loss_G = -22681880.0\n",
            "Epoch 6700: Loss_T = -23109360.0, Loss_G = -23052486.0\n",
            "Epoch 6800: Loss_T = -23455088.0, Loss_G = -23174544.0\n",
            "Epoch 6900: Loss_T = -23540642.0, Loss_G = -23645218.0\n",
            "Epoch 7000: Loss_T = -24317138.0, Loss_G = -24357452.0\n",
            "Epoch 7100: Loss_T = -24639748.0, Loss_G = -24522214.0\n",
            "Epoch 7200: Loss_T = -24724208.0, Loss_G = -25046316.0\n",
            "Epoch 7300: Loss_T = -25367882.0, Loss_G = -25405618.0\n",
            "Epoch 7400: Loss_T = -25367392.0, Loss_G = -25612568.0\n",
            "Epoch 7500: Loss_T = -26320686.0, Loss_G = -26187060.0\n",
            "Epoch 7600: Loss_T = -26181382.0, Loss_G = -26533486.0\n",
            "Epoch 7700: Loss_T = -26911344.0, Loss_G = -27014080.0\n",
            "Epoch 7800: Loss_T = -27445952.0, Loss_G = -27002838.0\n",
            "Epoch 7900: Loss_T = -27552658.0, Loss_G = -27618076.0\n",
            "Epoch 8000: Loss_T = -27895056.0, Loss_G = -27872594.0\n",
            "Epoch 8100: Loss_T = -28040390.0, Loss_G = -28605032.0\n",
            "Epoch 8200: Loss_T = -28639902.0, Loss_G = -28832164.0\n",
            "Epoch 8300: Loss_T = -29090346.0, Loss_G = -28981000.0\n",
            "Epoch 8400: Loss_T = -29322990.0, Loss_G = -29472998.0\n",
            "Epoch 8500: Loss_T = -29630834.0, Loss_G = -29562832.0\n",
            "Epoch 8600: Loss_T = -30201310.0, Loss_G = -30218228.0\n",
            "Epoch 8700: Loss_T = -30462074.0, Loss_G = -30507674.0\n",
            "Epoch 8800: Loss_T = -30854978.0, Loss_G = -30839580.0\n",
            "Epoch 8900: Loss_T = -31442772.0, Loss_G = -31210130.0\n",
            "Epoch 9000: Loss_T = -31454478.0, Loss_G = -32023684.0\n",
            "Epoch 9100: Loss_T = -32003266.0, Loss_G = -31932114.0\n",
            "Epoch 9200: Loss_T = -32379014.0, Loss_G = -32503360.0\n",
            "Epoch 9300: Loss_T = -32969646.0, Loss_G = -32339984.0\n",
            "Epoch 9400: Loss_T = -33031596.0, Loss_G = -32849238.0\n",
            "Epoch 9500: Loss_T = -33692904.0, Loss_G = -33574472.0\n",
            "Epoch 9600: Loss_T = -34213120.0, Loss_G = -33468888.0\n",
            "Epoch 9700: Loss_T = -33815556.0, Loss_G = -33896744.0\n",
            "Epoch 9800: Loss_T = -34345664.0, Loss_G = -34160576.0\n",
            "Epoch 9900: Loss_T = -34588716.0, Loss_G = -34595048.0\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.001, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.10544568300247192, Loss_G = -0.032309941947460175\n",
            "Epoch 100: Loss_T = -1.3855390548706055, Loss_G = -1.3232393264770508\n",
            "Epoch 200: Loss_T = -4.932069301605225, Loss_G = -5.116263389587402\n",
            "Epoch 300: Loss_T = -11.362995147705078, Loss_G = -11.56225299835205\n",
            "Epoch 400: Loss_T = -20.62468719482422, Loss_G = -20.82046890258789\n",
            "Epoch 500: Loss_T = -32.50545120239258, Loss_G = -32.970821380615234\n",
            "Epoch 600: Loss_T = -47.291194915771484, Loss_G = -47.43205642700195\n",
            "Epoch 700: Loss_T = -64.49555206298828, Loss_G = -64.74061584472656\n",
            "Epoch 800: Loss_T = -84.2345962524414, Loss_G = -84.12798309326172\n",
            "Epoch 900: Loss_T = -106.1139144897461, Loss_G = -106.06859588623047\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.001, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.06994789838790894, Loss_G = 0.12270563095808029\n",
            "Epoch 100: Loss_T = -1.3595564365386963, Loss_G = -1.4816135168075562\n",
            "Epoch 200: Loss_T = -5.313664436340332, Loss_G = -5.184874057769775\n",
            "Epoch 300: Loss_T = -11.571537971496582, Loss_G = -12.024269104003906\n",
            "Epoch 400: Loss_T = -21.250497817993164, Loss_G = -21.09850311279297\n",
            "Epoch 500: Loss_T = -33.14692306518555, Loss_G = -33.817787170410156\n",
            "Epoch 600: Loss_T = -47.91106033325195, Loss_G = -47.90264892578125\n",
            "Epoch 700: Loss_T = -64.49940490722656, Loss_G = -65.97994995117188\n",
            "Epoch 800: Loss_T = -84.18360900878906, Loss_G = -84.0682373046875\n",
            "Epoch 900: Loss_T = -106.53501892089844, Loss_G = -106.18062591552734\n",
            "Epoch 1000: Loss_T = -130.8614501953125, Loss_G = -131.07334899902344\n",
            "Epoch 1100: Loss_T = -154.73703002929688, Loss_G = -152.94375610351562\n",
            "Epoch 1200: Loss_T = -180.4792022705078, Loss_G = -178.5103759765625\n",
            "Epoch 1300: Loss_T = -204.64202880859375, Loss_G = -204.90005493164062\n",
            "Epoch 1400: Loss_T = -233.19459533691406, Loss_G = -233.75625610351562\n",
            "Epoch 1500: Loss_T = -262.3657531738281, Loss_G = -263.30047607421875\n",
            "Epoch 1600: Loss_T = -291.29248046875, Loss_G = -295.279052734375\n",
            "Epoch 1700: Loss_T = -324.3873596191406, Loss_G = -324.5777893066406\n",
            "Epoch 1800: Loss_T = -356.2837219238281, Loss_G = -356.8669128417969\n",
            "Epoch 1900: Loss_T = -392.28216552734375, Loss_G = -393.07757568359375\n",
            "Epoch 2000: Loss_T = -430.1201171875, Loss_G = -430.29266357421875\n",
            "Epoch 2100: Loss_T = -464.3788146972656, Loss_G = -461.55743408203125\n",
            "Epoch 2200: Loss_T = -496.0849609375, Loss_G = -496.88983154296875\n",
            "Epoch 2300: Loss_T = -530.8713989257812, Loss_G = -530.6845092773438\n",
            "Epoch 2400: Loss_T = -569.573486328125, Loss_G = -566.5155029296875\n",
            "Epoch 2500: Loss_T = -601.7510986328125, Loss_G = -606.3814697265625\n",
            "Epoch 2600: Loss_T = -642.227783203125, Loss_G = -643.3919677734375\n",
            "Epoch 2700: Loss_T = -681.3484497070312, Loss_G = -680.4957275390625\n",
            "Epoch 2800: Loss_T = -718.8567504882812, Loss_G = -719.4860229492188\n",
            "Epoch 2900: Loss_T = -756.7484741210938, Loss_G = -759.772216796875\n",
            "Epoch 3000: Loss_T = -803.7708129882812, Loss_G = -798.0970458984375\n",
            "Epoch 3100: Loss_T = -837.6132202148438, Loss_G = -837.6636962890625\n",
            "Epoch 3200: Loss_T = -883.2770385742188, Loss_G = -876.8158569335938\n",
            "Epoch 3300: Loss_T = -912.4544677734375, Loss_G = -918.346435546875\n",
            "Epoch 3400: Loss_T = -953.93408203125, Loss_G = -954.7711181640625\n",
            "Epoch 3500: Loss_T = -1005.0426025390625, Loss_G = -1003.4408569335938\n",
            "Epoch 3600: Loss_T = -1038.4951171875, Loss_G = -1043.1123046875\n",
            "Epoch 3700: Loss_T = -1086.3792724609375, Loss_G = -1074.2076416015625\n",
            "Epoch 3800: Loss_T = -1125.4144287109375, Loss_G = -1118.9176025390625\n",
            "Epoch 3900: Loss_T = -1165.4610595703125, Loss_G = -1157.2025146484375\n",
            "Epoch 4000: Loss_T = -1211.4686279296875, Loss_G = -1208.3668212890625\n",
            "Epoch 4100: Loss_T = -1245.242919921875, Loss_G = -1250.637451171875\n",
            "Epoch 4200: Loss_T = -1294.9600830078125, Loss_G = -1292.0880126953125\n",
            "Epoch 4300: Loss_T = -1324.4747314453125, Loss_G = -1325.755615234375\n",
            "Epoch 4400: Loss_T = -1373.23486328125, Loss_G = -1376.1036376953125\n",
            "Epoch 4500: Loss_T = -1401.572265625, Loss_G = -1419.360595703125\n",
            "Epoch 4600: Loss_T = -1448.2567138671875, Loss_G = -1454.3953857421875\n",
            "Epoch 4700: Loss_T = -1486.11962890625, Loss_G = -1492.262939453125\n",
            "Epoch 4800: Loss_T = -1526.366943359375, Loss_G = -1536.7762451171875\n",
            "Epoch 4900: Loss_T = -1575.5164794921875, Loss_G = -1571.6297607421875\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.001, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = 0.05119342356920242, Loss_G = 0.05694044008851051\n",
            "Epoch 100: Loss_T = -1.4268853664398193, Loss_G = -1.38777494430542\n",
            "Epoch 200: Loss_T = -4.994421482086182, Loss_G = -4.917544364929199\n",
            "Epoch 300: Loss_T = -11.365796089172363, Loss_G = -11.474714279174805\n",
            "Epoch 400: Loss_T = -20.855146408081055, Loss_G = -21.194711685180664\n",
            "Epoch 500: Loss_T = -33.07460021972656, Loss_G = -32.630977630615234\n",
            "Epoch 600: Loss_T = -47.349609375, Loss_G = -47.93767547607422\n",
            "Epoch 700: Loss_T = -64.34612274169922, Loss_G = -64.80751037597656\n",
            "Epoch 800: Loss_T = -83.23760223388672, Loss_G = -84.65203857421875\n",
            "Epoch 900: Loss_T = -106.32234191894531, Loss_G = -105.23699951171875\n",
            "Epoch 1000: Loss_T = -129.98011779785156, Loss_G = -130.283447265625\n",
            "Epoch 1100: Loss_T = -152.4498748779297, Loss_G = -154.24130249023438\n",
            "Epoch 1200: Loss_T = -178.4563751220703, Loss_G = -179.19064331054688\n",
            "Epoch 1300: Loss_T = -203.58445739746094, Loss_G = -206.060302734375\n",
            "Epoch 1400: Loss_T = -232.751708984375, Loss_G = -232.31053161621094\n",
            "Epoch 1500: Loss_T = -258.807861328125, Loss_G = -261.1759033203125\n",
            "Epoch 1600: Loss_T = -291.1767272949219, Loss_G = -293.1438903808594\n",
            "Epoch 1700: Loss_T = -321.6696472167969, Loss_G = -324.0187072753906\n",
            "Epoch 1800: Loss_T = -357.2714538574219, Loss_G = -359.9758605957031\n",
            "Epoch 1900: Loss_T = -390.60076904296875, Loss_G = -393.88720703125\n",
            "Epoch 2000: Loss_T = -426.6505432128906, Loss_G = -426.43731689453125\n",
            "Epoch 2100: Loss_T = -458.40118408203125, Loss_G = -460.9515380859375\n",
            "Epoch 2200: Loss_T = -497.04742431640625, Loss_G = -495.4985046386719\n",
            "Epoch 2300: Loss_T = -531.2430419921875, Loss_G = -530.2567138671875\n",
            "Epoch 2400: Loss_T = -564.7501831054688, Loss_G = -567.3356323242188\n",
            "Epoch 2500: Loss_T = -603.936767578125, Loss_G = -601.2736206054688\n",
            "Epoch 2600: Loss_T = -642.9937133789062, Loss_G = -642.8837890625\n",
            "Epoch 2700: Loss_T = -684.7846069335938, Loss_G = -680.1146850585938\n",
            "Epoch 2800: Loss_T = -718.7406616210938, Loss_G = -721.373779296875\n",
            "Epoch 2900: Loss_T = -758.3636474609375, Loss_G = -752.429931640625\n",
            "Epoch 3000: Loss_T = -801.3111572265625, Loss_G = -807.082763671875\n",
            "Epoch 3100: Loss_T = -844.6205444335938, Loss_G = -835.2216796875\n",
            "Epoch 3200: Loss_T = -883.100830078125, Loss_G = -879.00244140625\n",
            "Epoch 3300: Loss_T = -921.16015625, Loss_G = -914.3699340820312\n",
            "Epoch 3400: Loss_T = -961.10546875, Loss_G = -956.3749389648438\n",
            "Epoch 3500: Loss_T = -994.0166015625, Loss_G = -992.732177734375\n",
            "Epoch 3600: Loss_T = -1039.65234375, Loss_G = -1032.5731201171875\n",
            "Epoch 3700: Loss_T = -1075.0947265625, Loss_G = -1080.776611328125\n",
            "Epoch 3800: Loss_T = -1122.518310546875, Loss_G = -1117.34228515625\n",
            "Epoch 3900: Loss_T = -1163.194091796875, Loss_G = -1166.75048828125\n",
            "Epoch 4000: Loss_T = -1206.81005859375, Loss_G = -1201.0704345703125\n",
            "Epoch 4100: Loss_T = -1245.292236328125, Loss_G = -1238.237060546875\n",
            "Epoch 4200: Loss_T = -1298.0438232421875, Loss_G = -1277.7113037109375\n",
            "Epoch 4300: Loss_T = -1317.8958740234375, Loss_G = -1321.62939453125\n",
            "Epoch 4400: Loss_T = -1371.66650390625, Loss_G = -1364.378662109375\n",
            "Epoch 4500: Loss_T = -1411.9022216796875, Loss_G = -1409.4404296875\n",
            "Epoch 4600: Loss_T = -1448.995361328125, Loss_G = -1445.222900390625\n",
            "Epoch 4700: Loss_T = -1488.661865234375, Loss_G = -1485.200927734375\n",
            "Epoch 4800: Loss_T = -1532.6663818359375, Loss_G = -1535.403564453125\n",
            "Epoch 4900: Loss_T = -1576.7249755859375, Loss_G = -1574.68017578125\n",
            "Epoch 5000: Loss_T = -1611.6328125, Loss_G = -1622.706787109375\n",
            "Epoch 5100: Loss_T = -1650.0972900390625, Loss_G = -1668.032470703125\n",
            "Epoch 5200: Loss_T = -1706.6700439453125, Loss_G = -1704.8125\n",
            "Epoch 5300: Loss_T = -1736.0126953125, Loss_G = -1744.37890625\n",
            "Epoch 5400: Loss_T = -1773.8338623046875, Loss_G = -1786.680419921875\n",
            "Epoch 5500: Loss_T = -1810.215576171875, Loss_G = -1823.25390625\n",
            "Epoch 5600: Loss_T = -1866.658447265625, Loss_G = -1858.5645751953125\n",
            "Epoch 5700: Loss_T = -1902.392822265625, Loss_G = -1896.8394775390625\n",
            "Epoch 5800: Loss_T = -1947.885009765625, Loss_G = -1950.486083984375\n",
            "Epoch 5900: Loss_T = -1987.367431640625, Loss_G = -1987.5255126953125\n",
            "Epoch 6000: Loss_T = -2025.28466796875, Loss_G = -2030.542724609375\n",
            "Epoch 6100: Loss_T = -2062.67626953125, Loss_G = -2062.8720703125\n",
            "Epoch 6200: Loss_T = -2112.747802734375, Loss_G = -2103.69482421875\n",
            "Epoch 6300: Loss_T = -2150.2197265625, Loss_G = -2136.9033203125\n",
            "Epoch 6400: Loss_T = -2181.175537109375, Loss_G = -2200.9873046875\n",
            "Epoch 6500: Loss_T = -2236.86572265625, Loss_G = -2225.037109375\n",
            "Epoch 6600: Loss_T = -2272.16259765625, Loss_G = -2264.02685546875\n",
            "Epoch 6700: Loss_T = -2307.19775390625, Loss_G = -2318.135009765625\n",
            "Epoch 6800: Loss_T = -2340.83740234375, Loss_G = -2333.07470703125\n",
            "Epoch 6900: Loss_T = -2383.5888671875, Loss_G = -2396.5\n",
            "Epoch 7000: Loss_T = -2424.257568359375, Loss_G = -2432.5126953125\n",
            "Epoch 7100: Loss_T = -2471.615234375, Loss_G = -2473.623046875\n",
            "Epoch 7200: Loss_T = -2489.749755859375, Loss_G = -2500.67333984375\n",
            "Epoch 7300: Loss_T = -2562.83447265625, Loss_G = -2545.27099609375\n",
            "Epoch 7400: Loss_T = -2594.16162109375, Loss_G = -2580.5244140625\n",
            "Epoch 7500: Loss_T = -2587.15771484375, Loss_G = -2602.628662109375\n",
            "Epoch 7600: Loss_T = -2661.10986328125, Loss_G = -2649.03564453125\n",
            "Epoch 7700: Loss_T = -2694.852783203125, Loss_G = -2699.74072265625\n",
            "Epoch 7800: Loss_T = -2732.173095703125, Loss_G = -2742.050048828125\n",
            "Epoch 7900: Loss_T = -2760.514892578125, Loss_G = -2767.438232421875\n",
            "Epoch 8000: Loss_T = -2814.71044921875, Loss_G = -2813.08837890625\n",
            "Epoch 8100: Loss_T = -2850.1787109375, Loss_G = -2853.82373046875\n",
            "Epoch 8200: Loss_T = -2877.218994140625, Loss_G = -2883.1279296875\n",
            "Epoch 8300: Loss_T = -2921.901611328125, Loss_G = -2920.2587890625\n",
            "Epoch 8400: Loss_T = -2954.88623046875, Loss_G = -2961.6572265625\n",
            "Epoch 8500: Loss_T = -2987.6640625, Loss_G = -2993.775390625\n",
            "Epoch 8600: Loss_T = -3023.563720703125, Loss_G = -3032.5087890625\n",
            "Epoch 8700: Loss_T = -3051.69970703125, Loss_G = -3060.5771484375\n",
            "Epoch 8800: Loss_T = -3091.023681640625, Loss_G = -3111.8935546875\n",
            "Epoch 8900: Loss_T = -3134.011962890625, Loss_G = -3145.67041015625\n",
            "Epoch 9000: Loss_T = -3168.70361328125, Loss_G = -3171.393310546875\n",
            "Epoch 9100: Loss_T = -3209.361328125, Loss_G = -3206.543212890625\n",
            "Epoch 9200: Loss_T = -3230.5234375, Loss_G = -3248.94677734375\n",
            "Epoch 9300: Loss_T = -3260.669921875, Loss_G = -3293.112060546875\n",
            "Epoch 9400: Loss_T = -3289.19677734375, Loss_G = -3288.289306640625\n",
            "Epoch 9500: Loss_T = -3331.393798828125, Loss_G = -3334.13134765625\n",
            "Epoch 9600: Loss_T = -3401.082763671875, Loss_G = -3383.40283203125\n",
            "Epoch 9700: Loss_T = -3416.148193359375, Loss_G = -3431.53662109375\n",
            "Epoch 9800: Loss_T = -3461.98974609375, Loss_G = -3467.758544921875\n",
            "Epoch 9900: Loss_T = -3482.379150390625, Loss_G = -3504.708251953125\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.01, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.2385205626487732, Loss_G = 0.007245213259011507\n",
            "Epoch 100: Loss_T = -100.3206787109375, Loss_G = -104.64099884033203\n",
            "Epoch 200: Loss_T = -480.22564697265625, Loss_G = -482.78558349609375\n",
            "Epoch 300: Loss_T = -1145.558349609375, Loss_G = -1148.069580078125\n",
            "Epoch 400: Loss_T = -2091.4208984375, Loss_G = -2106.799072265625\n",
            "Epoch 500: Loss_T = -3316.31103515625, Loss_G = -3298.953125\n",
            "Epoch 600: Loss_T = -4762.94970703125, Loss_G = -4790.94140625\n",
            "Epoch 700: Loss_T = -6489.65576171875, Loss_G = -6545.01806640625\n",
            "Epoch 800: Loss_T = -8421.40234375, Loss_G = -8424.8779296875\n",
            "Epoch 900: Loss_T = -10685.603515625, Loss_G = -10671.2724609375\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.01, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.08044105023145676, Loss_G = -0.016204316169023514\n",
            "Epoch 100: Loss_T = -100.55027770996094, Loss_G = -102.84188842773438\n",
            "Epoch 200: Loss_T = -474.32073974609375, Loss_G = -483.3197326660156\n",
            "Epoch 300: Loss_T = -1148.90380859375, Loss_G = -1160.588623046875\n",
            "Epoch 400: Loss_T = -2095.46630859375, Loss_G = -2108.08837890625\n",
            "Epoch 500: Loss_T = -3315.613525390625, Loss_G = -3307.063720703125\n",
            "Epoch 600: Loss_T = -4780.29296875, Loss_G = -4777.642578125\n",
            "Epoch 700: Loss_T = -6460.30517578125, Loss_G = -6542.02392578125\n",
            "Epoch 800: Loss_T = -8388.81640625, Loss_G = -8490.0615234375\n",
            "Epoch 900: Loss_T = -10653.3603515625, Loss_G = -10659.0869140625\n",
            "Epoch 1000: Loss_T = -13104.994140625, Loss_G = -13015.876953125\n",
            "Epoch 1100: Loss_T = -15238.6083984375, Loss_G = -15334.044921875\n",
            "Epoch 1200: Loss_T = -17768.12890625, Loss_G = -17767.88671875\n",
            "Epoch 1300: Loss_T = -20501.7265625, Loss_G = -20398.37109375\n",
            "Epoch 1400: Loss_T = -23317.16796875, Loss_G = -23289.671875\n",
            "Epoch 1500: Loss_T = -26098.59375, Loss_G = -26110.421875\n",
            "Epoch 1600: Loss_T = -29297.9375, Loss_G = -29344.0625\n",
            "Epoch 1700: Loss_T = -32300.796875, Loss_G = -32374.14453125\n",
            "Epoch 1800: Loss_T = -35803.6640625, Loss_G = -35936.6953125\n",
            "Epoch 1900: Loss_T = -39285.015625, Loss_G = -39130.890625\n",
            "Epoch 2000: Loss_T = -42418.15625, Loss_G = -42591.77734375\n",
            "Epoch 2100: Loss_T = -45814.68359375, Loss_G = -46077.01953125\n",
            "Epoch 2200: Loss_T = -49677.6171875, Loss_G = -49576.68359375\n",
            "Epoch 2300: Loss_T = -52780.640625, Loss_G = -53026.046875\n",
            "Epoch 2400: Loss_T = -56417.2734375, Loss_G = -56173.24609375\n",
            "Epoch 2500: Loss_T = -60281.79296875, Loss_G = -60782.8359375\n",
            "Epoch 2600: Loss_T = -64097.8828125, Loss_G = -64187.44921875\n",
            "Epoch 2700: Loss_T = -67814.375, Loss_G = -68103.5703125\n",
            "Epoch 2800: Loss_T = -71742.1328125, Loss_G = -72699.0078125\n",
            "Epoch 2900: Loss_T = -76264.546875, Loss_G = -76255.484375\n",
            "Epoch 3000: Loss_T = -80176.75, Loss_G = -80171.4453125\n",
            "Epoch 3100: Loss_T = -83746.5390625, Loss_G = -83981.046875\n",
            "Epoch 3200: Loss_T = -87752.609375, Loss_G = -88173.8828125\n",
            "Epoch 3300: Loss_T = -91296.125, Loss_G = -90400.09375\n",
            "Epoch 3400: Loss_T = -94734.984375, Loss_G = -94928.65625\n",
            "Epoch 3500: Loss_T = -100634.1015625, Loss_G = -99347.78125\n",
            "Epoch 3600: Loss_T = -103338.765625, Loss_G = -103350.84375\n",
            "Epoch 3700: Loss_T = -107711.46875, Loss_G = -107449.859375\n",
            "Epoch 3800: Loss_T = -111799.8515625, Loss_G = -112379.75\n",
            "Epoch 3900: Loss_T = -116290.421875, Loss_G = -116510.1015625\n",
            "Epoch 4000: Loss_T = -120409.4375, Loss_G = -120222.9296875\n",
            "Epoch 4100: Loss_T = -124803.640625, Loss_G = -123851.9296875\n",
            "Epoch 4200: Loss_T = -128084.796875, Loss_G = -128990.5390625\n",
            "Epoch 4300: Loss_T = -132302.375, Loss_G = -132396.71875\n",
            "Epoch 4400: Loss_T = -136061.9375, Loss_G = -137345.640625\n",
            "Epoch 4500: Loss_T = -139903.15625, Loss_G = -141137.59375\n",
            "Epoch 4600: Loss_T = -144344.0625, Loss_G = -144574.578125\n",
            "Epoch 4700: Loss_T = -149500.453125, Loss_G = -150150.375\n",
            "Epoch 4800: Loss_T = -152881.484375, Loss_G = -152842.875\n",
            "Epoch 4900: Loss_T = -156265.1875, Loss_G = -156694.25\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.01, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = 0.0933094173669815, Loss_G = -0.0276743546128273\n",
            "Epoch 100: Loss_T = -101.41671752929688, Loss_G = -104.04659271240234\n",
            "Epoch 200: Loss_T = -477.124267578125, Loss_G = -486.5641784667969\n",
            "Epoch 300: Loss_T = -1141.6710205078125, Loss_G = -1150.874755859375\n",
            "Epoch 400: Loss_T = -2087.774169921875, Loss_G = -2100.6162109375\n",
            "Epoch 500: Loss_T = -3320.16845703125, Loss_G = -3303.671875\n",
            "Epoch 600: Loss_T = -4771.2841796875, Loss_G = -4782.0361328125\n",
            "Epoch 700: Loss_T = -6461.07958984375, Loss_G = -6517.59521484375\n",
            "Epoch 800: Loss_T = -8431.580078125, Loss_G = -8404.666015625\n",
            "Epoch 900: Loss_T = -10614.19140625, Loss_G = -10598.134765625\n",
            "Epoch 1000: Loss_T = -12961.6767578125, Loss_G = -13101.109375\n",
            "Epoch 1100: Loss_T = -15308.1259765625, Loss_G = -15447.8037109375\n",
            "Epoch 1200: Loss_T = -17955.34375, Loss_G = -17836.359375\n",
            "Epoch 1300: Loss_T = -20391.62109375, Loss_G = -20491.37890625\n",
            "Epoch 1400: Loss_T = -23347.96875, Loss_G = -23286.24609375\n",
            "Epoch 1500: Loss_T = -26065.2890625, Loss_G = -26185.7578125\n",
            "Epoch 1600: Loss_T = -29001.94921875, Loss_G = -29251.2578125\n",
            "Epoch 1700: Loss_T = -32403.59765625, Loss_G = -32314.64453125\n",
            "Epoch 1800: Loss_T = -35767.05859375, Loss_G = -35522.8203125\n",
            "Epoch 1900: Loss_T = -39096.72265625, Loss_G = -38981.71875\n",
            "Epoch 2000: Loss_T = -42702.625, Loss_G = -42908.0546875\n",
            "Epoch 2100: Loss_T = -45957.1953125, Loss_G = -45975.04296875\n",
            "Epoch 2200: Loss_T = -49554.4375, Loss_G = -49255.76171875\n",
            "Epoch 2300: Loss_T = -52937.33203125, Loss_G = -53716.953125\n",
            "Epoch 2400: Loss_T = -55868.21875, Loss_G = -56547.87109375\n",
            "Epoch 2500: Loss_T = -60014.3046875, Loss_G = -60427.921875\n",
            "Epoch 2600: Loss_T = -63737.19921875, Loss_G = -64015.140625\n",
            "Epoch 2700: Loss_T = -67515.6640625, Loss_G = -68109.5703125\n",
            "Epoch 2800: Loss_T = -71911.984375, Loss_G = -71518.171875\n",
            "Epoch 2900: Loss_T = -76503.671875, Loss_G = -75775.2265625\n",
            "Epoch 3000: Loss_T = -79795.7734375, Loss_G = -80504.5703125\n",
            "Epoch 3100: Loss_T = -83493.1875, Loss_G = -84196.4375\n",
            "Epoch 3200: Loss_T = -87558.0234375, Loss_G = -88224.3359375\n",
            "Epoch 3300: Loss_T = -90964.7578125, Loss_G = -92377.0703125\n",
            "Epoch 3400: Loss_T = -95782.9140625, Loss_G = -95650.84375\n",
            "Epoch 3500: Loss_T = -100053.6171875, Loss_G = -99520.4921875\n",
            "Epoch 3600: Loss_T = -103673.5, Loss_G = -104105.59375\n",
            "Epoch 3700: Loss_T = -108946.203125, Loss_G = -107785.09375\n",
            "Epoch 3800: Loss_T = -111575.453125, Loss_G = -112162.7578125\n",
            "Epoch 3900: Loss_T = -116264.125, Loss_G = -116142.6796875\n",
            "Epoch 4000: Loss_T = -120330.1875, Loss_G = -119755.875\n",
            "Epoch 4100: Loss_T = -123921.859375, Loss_G = -123773.5625\n",
            "Epoch 4200: Loss_T = -128844.6171875, Loss_G = -128125.015625\n",
            "Epoch 4300: Loss_T = -132471.8125, Loss_G = -131224.234375\n",
            "Epoch 4400: Loss_T = -135746.625, Loss_G = -135908.0625\n",
            "Epoch 4500: Loss_T = -140246.1875, Loss_G = -141498.75\n",
            "Epoch 4600: Loss_T = -144438.5, Loss_G = -145054.484375\n",
            "Epoch 4700: Loss_T = -149013.5, Loss_G = -148522.28125\n",
            "Epoch 4800: Loss_T = -152622.75, Loss_G = -153237.953125\n",
            "Epoch 4900: Loss_T = -156696.25, Loss_G = -156051.65625\n",
            "Epoch 5000: Loss_T = -162022.1875, Loss_G = -161358.609375\n",
            "Epoch 5100: Loss_T = -166922.96875, Loss_G = -166238.703125\n",
            "Epoch 5200: Loss_T = -168727.3125, Loss_G = -169820.40625\n",
            "Epoch 5300: Loss_T = -174499.40625, Loss_G = -174399.796875\n",
            "Epoch 5400: Loss_T = -177724.96875, Loss_G = -178058.4375\n",
            "Epoch 5500: Loss_T = -180667.109375, Loss_G = -181798.3125\n",
            "Epoch 5600: Loss_T = -186622.21875, Loss_G = -186412.453125\n",
            "Epoch 5700: Loss_T = -190058.28125, Loss_G = -189455.484375\n",
            "Epoch 5800: Loss_T = -193235.671875, Loss_G = -193602.859375\n",
            "Epoch 5900: Loss_T = -198646.34375, Loss_G = -199015.4375\n",
            "Epoch 6000: Loss_T = -203325.609375, Loss_G = -203001.0625\n",
            "Epoch 6100: Loss_T = -207866.859375, Loss_G = -206764.90625\n",
            "Epoch 6200: Loss_T = -208682.859375, Loss_G = -209749.3125\n",
            "Epoch 6300: Loss_T = -215049.921875, Loss_G = -214456.421875\n",
            "Epoch 6400: Loss_T = -217725.609375, Loss_G = -217774.1875\n",
            "Epoch 6500: Loss_T = -222471.328125, Loss_G = -222197.0625\n",
            "Epoch 6600: Loss_T = -226222.828125, Loss_G = -227427.84375\n",
            "Epoch 6700: Loss_T = -229952.125, Loss_G = -229900.8125\n",
            "Epoch 6800: Loss_T = -233398.375, Loss_G = -235059.046875\n",
            "Epoch 6900: Loss_T = -236903.9375, Loss_G = -238845.8125\n",
            "Epoch 7000: Loss_T = -241538.796875, Loss_G = -241866.734375\n",
            "Epoch 7100: Loss_T = -244957.703125, Loss_G = -248930.6875\n",
            "Epoch 7200: Loss_T = -248905.921875, Loss_G = -249545.609375\n",
            "Epoch 7300: Loss_T = -254655.515625, Loss_G = -253346.140625\n",
            "Epoch 7400: Loss_T = -257865.828125, Loss_G = -256169.53125\n",
            "Epoch 7500: Loss_T = -262348.125, Loss_G = -261375.09375\n",
            "Epoch 7600: Loss_T = -266109.5, Loss_G = -266831.03125\n",
            "Epoch 7700: Loss_T = -266809.09375, Loss_G = -269916.75\n",
            "Epoch 7800: Loss_T = -272716.3125, Loss_G = -271787.9375\n",
            "Epoch 7900: Loss_T = -277527.9375, Loss_G = -275955.21875\n",
            "Epoch 8000: Loss_T = -280523.84375, Loss_G = -280016.875\n",
            "Epoch 8100: Loss_T = -284625.84375, Loss_G = -284359.53125\n",
            "Epoch 8200: Loss_T = -287098.3125, Loss_G = -287212.5625\n",
            "Epoch 8300: Loss_T = -290980.125, Loss_G = -292407.1875\n",
            "Epoch 8400: Loss_T = -294370.625, Loss_G = -294078.5625\n",
            "Epoch 8500: Loss_T = -297722.40625, Loss_G = -298865.71875\n",
            "Epoch 8600: Loss_T = -305498.25, Loss_G = -302514.875\n",
            "Epoch 8700: Loss_T = -305905.8125, Loss_G = -304734.09375\n",
            "Epoch 8800: Loss_T = -310090.03125, Loss_G = -309156.5\n",
            "Epoch 8900: Loss_T = -311244.9375, Loss_G = -313272.875\n",
            "Epoch 9000: Loss_T = -314617.0, Loss_G = -317831.71875\n",
            "Epoch 9100: Loss_T = -320388.46875, Loss_G = -320808.53125\n",
            "Epoch 9200: Loss_T = -324614.53125, Loss_G = -324458.5\n",
            "Epoch 9300: Loss_T = -326493.1875, Loss_G = -327028.875\n",
            "Epoch 9400: Loss_T = -329462.875, Loss_G = -331132.0625\n",
            "Epoch 9500: Loss_T = -334057.90625, Loss_G = -331295.1875\n",
            "Epoch 9600: Loss_T = -336500.90625, Loss_G = -339872.71875\n",
            "Epoch 9700: Loss_T = -340159.71875, Loss_G = -342019.125\n",
            "Epoch 9800: Loss_T = -346033.875, Loss_G = -341092.40625\n",
            "Epoch 9900: Loss_T = -348542.78125, Loss_G = -346983.8125\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.1, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.16571904718875885, Loss_G = -0.8824770450592041\n",
            "Epoch 100: Loss_T = -9664.5234375, Loss_G = -10000.0244140625\n",
            "Epoch 200: Loss_T = -47444.19140625, Loss_G = -48098.1875\n",
            "Epoch 300: Loss_T = -114002.21875, Loss_G = -115430.1328125\n",
            "Epoch 400: Loss_T = -209476.46875, Loss_G = -209636.421875\n",
            "Epoch 500: Loss_T = -329079.78125, Loss_G = -330656.0625\n",
            "Epoch 600: Loss_T = -477037.71875, Loss_G = -480582.25\n",
            "Epoch 700: Loss_T = -650092.375, Loss_G = -647410.5625\n",
            "Epoch 800: Loss_T = -838092.625, Loss_G = -841823.0625\n",
            "Epoch 900: Loss_T = -1058019.625, Loss_G = -1060767.625\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.1, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.10987958312034607, Loss_G = -0.7361316084861755\n",
            "Epoch 100: Loss_T = -9841.7763671875, Loss_G = -10027.603515625\n",
            "Epoch 200: Loss_T = -47773.41015625, Loss_G = -48455.8515625\n",
            "Epoch 300: Loss_T = -114414.609375, Loss_G = -115207.3671875\n",
            "Epoch 400: Loss_T = -209385.484375, Loss_G = -208987.390625\n",
            "Epoch 500: Loss_T = -332486.84375, Loss_G = -332161.5\n",
            "Epoch 600: Loss_T = -475200.125, Loss_G = -479784.5\n",
            "Epoch 700: Loss_T = -644518.125, Loss_G = -649388.0625\n",
            "Epoch 800: Loss_T = -845554.375, Loss_G = -842609.625\n",
            "Epoch 900: Loss_T = -1057485.875, Loss_G = -1070201.875\n",
            "Epoch 1000: Loss_T = -1303120.25, Loss_G = -1297539.75\n",
            "Epoch 1100: Loss_T = -1528906.5, Loss_G = -1524528.5\n",
            "Epoch 1200: Loss_T = -1774485.0, Loss_G = -1780443.5\n",
            "Epoch 1300: Loss_T = -2039219.75, Loss_G = -2042610.75\n",
            "Epoch 1400: Loss_T = -2325902.75, Loss_G = -2319003.25\n",
            "Epoch 1500: Loss_T = -2605721.5, Loss_G = -2610487.0\n",
            "Epoch 1600: Loss_T = -2925152.75, Loss_G = -2902025.5\n",
            "Epoch 1700: Loss_T = -3236606.0, Loss_G = -3229261.75\n",
            "Epoch 1800: Loss_T = -3582201.25, Loss_G = -3565460.5\n",
            "Epoch 1900: Loss_T = -3906556.5, Loss_G = -3920522.5\n",
            "Epoch 2000: Loss_T = -4268159.0, Loss_G = -4277675.0\n",
            "Epoch 2100: Loss_T = -4587832.5, Loss_G = -4608813.0\n",
            "Epoch 2200: Loss_T = -4963318.0, Loss_G = -4975966.5\n",
            "Epoch 2300: Loss_T = -5284812.0, Loss_G = -5333114.5\n",
            "Epoch 2400: Loss_T = -5645410.0, Loss_G = -5653853.0\n",
            "Epoch 2500: Loss_T = -6013627.0, Loss_G = -6030209.5\n",
            "Epoch 2600: Loss_T = -6371681.0, Loss_G = -6416019.5\n",
            "Epoch 2700: Loss_T = -6817289.5, Loss_G = -6800627.0\n",
            "Epoch 2800: Loss_T = -7231220.0, Loss_G = -7201840.5\n",
            "Epoch 2900: Loss_T = -7523708.0, Loss_G = -7608993.5\n",
            "Epoch 3000: Loss_T = -7918551.5, Loss_G = -8015700.5\n",
            "Epoch 3100: Loss_T = -8344230.0, Loss_G = -8426912.0\n",
            "Epoch 3200: Loss_T = -8839934.0, Loss_G = -8756475.0\n",
            "Epoch 3300: Loss_T = -9130830.0, Loss_G = -9135935.0\n",
            "Epoch 3400: Loss_T = -9565402.0, Loss_G = -9549519.0\n",
            "Epoch 3500: Loss_T = -9952647.0, Loss_G = -9941057.0\n",
            "Epoch 3600: Loss_T = -10308693.0, Loss_G = -10354994.0\n",
            "Epoch 3700: Loss_T = -10700888.0, Loss_G = -10829588.0\n",
            "Epoch 3800: Loss_T = -11183290.0, Loss_G = -11167318.0\n",
            "Epoch 3900: Loss_T = -11548567.0, Loss_G = -11590783.0\n",
            "Epoch 4000: Loss_T = -11989433.0, Loss_G = -12064792.0\n",
            "Epoch 4100: Loss_T = -12386206.0, Loss_G = -12457676.0\n",
            "Epoch 4200: Loss_T = -12860827.0, Loss_G = -12913111.0\n",
            "Epoch 4300: Loss_T = -13288328.0, Loss_G = -13225346.0\n",
            "Epoch 4400: Loss_T = -13679249.0, Loss_G = -13664394.0\n",
            "Epoch 4500: Loss_T = -14052336.0, Loss_G = -13986372.0\n",
            "Epoch 4600: Loss_T = -14388723.0, Loss_G = -14446959.0\n",
            "Epoch 4700: Loss_T = -14921470.0, Loss_G = -14909808.0\n",
            "Epoch 4800: Loss_T = -15350312.0, Loss_G = -15318514.0\n",
            "Epoch 4900: Loss_T = -15661534.0, Loss_G = -15791553.0\n",
            "Training with params: {'batch_size': 1024, 'learning_rate': 0.1, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = 0.10845591127872467, Loss_G = -0.8753588795661926\n",
            "Epoch 100: Loss_T = -9686.4228515625, Loss_G = -9967.146484375\n",
            "Epoch 200: Loss_T = -47532.5703125, Loss_G = -48378.828125\n",
            "Epoch 300: Loss_T = -114231.21875, Loss_G = -115617.6484375\n",
            "Epoch 400: Loss_T = -209342.296875, Loss_G = -210033.1875\n",
            "Epoch 500: Loss_T = -326787.90625, Loss_G = -331670.71875\n",
            "Epoch 600: Loss_T = -475945.875, Loss_G = -478653.15625\n",
            "Epoch 700: Loss_T = -648432.8125, Loss_G = -650151.375\n",
            "Epoch 800: Loss_T = -843835.625, Loss_G = -845998.0\n",
            "Epoch 900: Loss_T = -1064369.125, Loss_G = -1060059.0\n",
            "Epoch 1000: Loss_T = -1302580.5, Loss_G = -1300283.5\n",
            "Epoch 1100: Loss_T = -1527484.25, Loss_G = -1546000.625\n",
            "Epoch 1200: Loss_T = -1780450.625, Loss_G = -1782709.625\n",
            "Epoch 1300: Loss_T = -2039066.25, Loss_G = -2059413.5\n",
            "Epoch 1400: Loss_T = -2327742.0, Loss_G = -2329291.5\n",
            "Epoch 1500: Loss_T = -2607359.5, Loss_G = -2604714.0\n",
            "Epoch 1600: Loss_T = -2915519.5, Loss_G = -2946508.5\n",
            "Epoch 1700: Loss_T = -3245003.75, Loss_G = -3246723.25\n",
            "Epoch 1800: Loss_T = -3572081.75, Loss_G = -3564716.5\n",
            "Epoch 1900: Loss_T = -3923985.0, Loss_G = -3910520.75\n",
            "Epoch 2000: Loss_T = -4280830.0, Loss_G = -4263535.0\n",
            "Epoch 2100: Loss_T = -4577277.0, Loss_G = -4621503.0\n",
            "Epoch 2200: Loss_T = -4929669.0, Loss_G = -4971927.5\n",
            "Epoch 2300: Loss_T = -5264046.0, Loss_G = -5290581.5\n",
            "Epoch 2400: Loss_T = -5681582.0, Loss_G = -5659891.0\n",
            "Epoch 2500: Loss_T = -5990992.5, Loss_G = -6005932.0\n",
            "Epoch 2600: Loss_T = -6426146.0, Loss_G = -6412168.0\n",
            "Epoch 2700: Loss_T = -6797081.0, Loss_G = -6768624.0\n",
            "Epoch 2800: Loss_T = -7170483.0, Loss_G = -7231137.0\n",
            "Epoch 2900: Loss_T = -7588529.0, Loss_G = -7573012.0\n",
            "Epoch 3000: Loss_T = -7978824.0, Loss_G = -8031290.5\n",
            "Epoch 3100: Loss_T = -8390722.0, Loss_G = -8423962.0\n",
            "Epoch 3200: Loss_T = -8709708.0, Loss_G = -8775404.0\n",
            "Epoch 3300: Loss_T = -9180279.0, Loss_G = -9143987.0\n",
            "Epoch 3400: Loss_T = -9563156.0, Loss_G = -9545732.0\n",
            "Epoch 3500: Loss_T = -10004780.0, Loss_G = -9991993.0\n",
            "Epoch 3600: Loss_T = -10385601.0, Loss_G = -10374949.0\n",
            "Epoch 3700: Loss_T = -10792155.0, Loss_G = -10737906.0\n",
            "Epoch 3800: Loss_T = -11168664.0, Loss_G = -11199533.0\n",
            "Epoch 3900: Loss_T = -11641732.0, Loss_G = -11614465.0\n",
            "Epoch 4000: Loss_T = -12050204.0, Loss_G = -12001220.0\n",
            "Epoch 4100: Loss_T = -12410678.0, Loss_G = -12445224.0\n",
            "Epoch 4200: Loss_T = -12787556.0, Loss_G = -12798892.0\n",
            "Epoch 4300: Loss_T = -13308260.0, Loss_G = -13322966.0\n",
            "Epoch 4400: Loss_T = -13639231.0, Loss_G = -13656609.0\n",
            "Epoch 4500: Loss_T = -14051302.0, Loss_G = -14102882.0\n",
            "Epoch 4600: Loss_T = -14437806.0, Loss_G = -14515116.0\n",
            "Epoch 4700: Loss_T = -14952067.0, Loss_G = -14892248.0\n",
            "Epoch 4800: Loss_T = -15322392.0, Loss_G = -15281357.0\n",
            "Epoch 4900: Loss_T = -15567793.0, Loss_G = -15801799.0\n",
            "Epoch 5000: Loss_T = -16197525.0, Loss_G = -16136992.0\n",
            "Epoch 5100: Loss_T = -16615336.0, Loss_G = -16541962.0\n",
            "Epoch 5200: Loss_T = -16822276.0, Loss_G = -17066364.0\n",
            "Epoch 5300: Loss_T = -17339746.0, Loss_G = -17445616.0\n",
            "Epoch 5400: Loss_T = -17701608.0, Loss_G = -17815172.0\n",
            "Epoch 5500: Loss_T = -18092030.0, Loss_G = -18190568.0\n",
            "Epoch 5600: Loss_T = -18529106.0, Loss_G = -18641180.0\n",
            "Epoch 5700: Loss_T = -18950890.0, Loss_G = -18889696.0\n",
            "Epoch 5800: Loss_T = -19474396.0, Loss_G = -19404140.0\n",
            "Epoch 5900: Loss_T = -19823980.0, Loss_G = -19917316.0\n",
            "Epoch 6000: Loss_T = -20251722.0, Loss_G = -20159846.0\n",
            "Epoch 6100: Loss_T = -20751124.0, Loss_G = -20734276.0\n",
            "Epoch 6200: Loss_T = -21032644.0, Loss_G = -21156802.0\n",
            "Epoch 6300: Loss_T = -21479484.0, Loss_G = -21317012.0\n",
            "Epoch 6400: Loss_T = -21914118.0, Loss_G = -21891806.0\n",
            "Epoch 6500: Loss_T = -22124308.0, Loss_G = -22361104.0\n",
            "Epoch 6600: Loss_T = -22500802.0, Loss_G = -22674752.0\n",
            "Epoch 6700: Loss_T = -23042870.0, Loss_G = -22851046.0\n",
            "Epoch 6800: Loss_T = -23448550.0, Loss_G = -23514232.0\n",
            "Epoch 6900: Loss_T = -23864660.0, Loss_G = -23994668.0\n",
            "Epoch 7000: Loss_T = -24307028.0, Loss_G = -24181926.0\n",
            "Epoch 7100: Loss_T = -24521224.0, Loss_G = -24565864.0\n",
            "Epoch 7200: Loss_T = -24970202.0, Loss_G = -25036824.0\n",
            "Epoch 7300: Loss_T = -25392144.0, Loss_G = -25349132.0\n",
            "Epoch 7400: Loss_T = -25621872.0, Loss_G = -25846838.0\n",
            "Epoch 7500: Loss_T = -26222244.0, Loss_G = -26194032.0\n",
            "Epoch 7600: Loss_T = -26452582.0, Loss_G = -26599114.0\n",
            "Epoch 7700: Loss_T = -26998810.0, Loss_G = -27035544.0\n",
            "Epoch 7800: Loss_T = -27331360.0, Loss_G = -27425814.0\n",
            "Epoch 7900: Loss_T = -27683616.0, Loss_G = -27703540.0\n",
            "Epoch 8000: Loss_T = -28210998.0, Loss_G = -28100588.0\n",
            "Epoch 8100: Loss_T = -28511904.0, Loss_G = -28429680.0\n",
            "Epoch 8200: Loss_T = -28775792.0, Loss_G = -28617418.0\n",
            "Epoch 8300: Loss_T = -29170808.0, Loss_G = -29217980.0\n",
            "Epoch 8400: Loss_T = -29533762.0, Loss_G = -29501104.0\n",
            "Epoch 8500: Loss_T = -29861490.0, Loss_G = -29883578.0\n",
            "Epoch 8600: Loss_T = -30246712.0, Loss_G = -30292506.0\n",
            "Epoch 8700: Loss_T = -30481318.0, Loss_G = -30677908.0\n",
            "Epoch 8800: Loss_T = -30945402.0, Loss_G = -30949608.0\n",
            "Epoch 8900: Loss_T = -31425696.0, Loss_G = -31376086.0\n",
            "Epoch 9000: Loss_T = -31551672.0, Loss_G = -31499314.0\n",
            "Epoch 9100: Loss_T = -32019118.0, Loss_G = -32233126.0\n",
            "Epoch 9200: Loss_T = -32528720.0, Loss_G = -32321054.0\n",
            "Epoch 9300: Loss_T = -33002052.0, Loss_G = -32905070.0\n",
            "Epoch 9400: Loss_T = -33120524.0, Loss_G = -32959996.0\n",
            "Epoch 9500: Loss_T = -33413010.0, Loss_G = -33211200.0\n",
            "Epoch 9600: Loss_T = -33805244.0, Loss_G = -33833832.0\n",
            "Epoch 9700: Loss_T = -34194632.0, Loss_G = -34201532.0\n",
            "Epoch 9800: Loss_T = -34337268.0, Loss_G = -34284180.0\n",
            "Epoch 9900: Loss_T = -34658272.0, Loss_G = -34779992.0\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.001, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.039330512285232544, Loss_G = -0.0369647778570652\n",
            "Epoch 100: Loss_T = -1.682577133178711, Loss_G = -1.6957635879516602\n",
            "Epoch 200: Loss_T = -5.509103298187256, Loss_G = -5.68068790435791\n",
            "Epoch 300: Loss_T = -12.480480194091797, Loss_G = -12.498239517211914\n",
            "Epoch 400: Loss_T = -21.854923248291016, Loss_G = -22.15276336669922\n",
            "Epoch 500: Loss_T = -34.19646072387695, Loss_G = -34.22811508178711\n",
            "Epoch 600: Loss_T = -48.43717956542969, Loss_G = -48.913047790527344\n",
            "Epoch 700: Loss_T = -66.1488037109375, Loss_G = -66.19999694824219\n",
            "Epoch 800: Loss_T = -85.72518157958984, Loss_G = -85.50765991210938\n",
            "Epoch 900: Loss_T = -107.48344421386719, Loss_G = -107.89249420166016\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.001, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.04365397244691849, Loss_G = 0.056404173374176025\n",
            "Epoch 100: Loss_T = -1.49284827709198, Loss_G = -1.3953810930252075\n",
            "Epoch 200: Loss_T = -5.38316535949707, Loss_G = -5.38916015625\n",
            "Epoch 300: Loss_T = -12.083420753479004, Loss_G = -12.181269645690918\n",
            "Epoch 400: Loss_T = -21.49199867248535, Loss_G = -21.548057556152344\n",
            "Epoch 500: Loss_T = -33.74775695800781, Loss_G = -33.82003402709961\n",
            "Epoch 600: Loss_T = -48.08186340332031, Loss_G = -48.107398986816406\n",
            "Epoch 700: Loss_T = -65.69596862792969, Loss_G = -65.728515625\n",
            "Epoch 800: Loss_T = -84.91695404052734, Loss_G = -85.2655258178711\n",
            "Epoch 900: Loss_T = -106.80644989013672, Loss_G = -107.05455017089844\n",
            "Epoch 1000: Loss_T = -131.1049346923828, Loss_G = -130.96121215820312\n",
            "Epoch 1100: Loss_T = -153.38308715820312, Loss_G = -154.19287109375\n",
            "Epoch 1200: Loss_T = -179.20126342773438, Loss_G = -179.90960693359375\n",
            "Epoch 1300: Loss_T = -205.4473419189453, Loss_G = -206.1597442626953\n",
            "Epoch 1400: Loss_T = -232.83297729492188, Loss_G = -233.21446228027344\n",
            "Epoch 1500: Loss_T = -262.4368896484375, Loss_G = -263.25372314453125\n",
            "Epoch 1600: Loss_T = -293.8457336425781, Loss_G = -292.91497802734375\n",
            "Epoch 1700: Loss_T = -326.67523193359375, Loss_G = -326.17303466796875\n",
            "Epoch 1800: Loss_T = -359.3407897949219, Loss_G = -359.7166442871094\n",
            "Epoch 1900: Loss_T = -393.9781494140625, Loss_G = -392.7054443359375\n",
            "Epoch 2000: Loss_T = -429.54730224609375, Loss_G = -428.9267272949219\n",
            "Epoch 2100: Loss_T = -463.896484375, Loss_G = -463.65728759765625\n",
            "Epoch 2200: Loss_T = -498.1866149902344, Loss_G = -499.3282775878906\n",
            "Epoch 2300: Loss_T = -535.2884521484375, Loss_G = -530.9979858398438\n",
            "Epoch 2400: Loss_T = -570.7896118164062, Loss_G = -569.583740234375\n",
            "Epoch 2500: Loss_T = -608.5275268554688, Loss_G = -603.5458984375\n",
            "Epoch 2600: Loss_T = -644.0126953125, Loss_G = -644.8768310546875\n",
            "Epoch 2700: Loss_T = -681.4666748046875, Loss_G = -682.4442138671875\n",
            "Epoch 2800: Loss_T = -723.7318115234375, Loss_G = -722.0944213867188\n",
            "Epoch 2900: Loss_T = -760.8635864257812, Loss_G = -762.569091796875\n",
            "Epoch 3000: Loss_T = -802.0467529296875, Loss_G = -805.0855712890625\n",
            "Epoch 3100: Loss_T = -844.4282836914062, Loss_G = -844.4384155273438\n",
            "Epoch 3200: Loss_T = -881.4328002929688, Loss_G = -881.6633911132812\n",
            "Epoch 3300: Loss_T = -918.0580444335938, Loss_G = -920.2700805664062\n",
            "Epoch 3400: Loss_T = -950.499267578125, Loss_G = -957.1117553710938\n",
            "Epoch 3500: Loss_T = -1001.5172119140625, Loss_G = -1000.033203125\n",
            "Epoch 3600: Loss_T = -1037.884521484375, Loss_G = -1037.4644775390625\n",
            "Epoch 3700: Loss_T = -1080.6497802734375, Loss_G = -1080.16796875\n",
            "Epoch 3800: Loss_T = -1122.44970703125, Loss_G = -1120.8607177734375\n",
            "Epoch 3900: Loss_T = -1162.0767822265625, Loss_G = -1164.4814453125\n",
            "Epoch 4000: Loss_T = -1211.119873046875, Loss_G = -1207.8232421875\n",
            "Epoch 4100: Loss_T = -1250.54736328125, Loss_G = -1245.6038818359375\n",
            "Epoch 4200: Loss_T = -1289.0408935546875, Loss_G = -1286.0458984375\n",
            "Epoch 4300: Loss_T = -1337.7293701171875, Loss_G = -1331.899169921875\n",
            "Epoch 4400: Loss_T = -1365.23291015625, Loss_G = -1372.0145263671875\n",
            "Epoch 4500: Loss_T = -1402.3199462890625, Loss_G = -1412.9102783203125\n",
            "Epoch 4600: Loss_T = -1453.75146484375, Loss_G = -1445.8126220703125\n",
            "Epoch 4700: Loss_T = -1497.0267333984375, Loss_G = -1490.69091796875\n",
            "Epoch 4800: Loss_T = -1537.7220458984375, Loss_G = -1533.04638671875\n",
            "Epoch 4900: Loss_T = -1585.6492919921875, Loss_G = -1577.3460693359375\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.001, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = -0.08893395960330963, Loss_G = -0.11214755475521088\n",
            "Epoch 100: Loss_T = -1.5855854749679565, Loss_G = -1.624169111251831\n",
            "Epoch 200: Loss_T = -5.440239429473877, Loss_G = -5.603241443634033\n",
            "Epoch 300: Loss_T = -12.18397331237793, Loss_G = -12.48686408996582\n",
            "Epoch 400: Loss_T = -21.627849578857422, Loss_G = -21.6685733795166\n",
            "Epoch 500: Loss_T = -33.39016342163086, Loss_G = -34.148799896240234\n",
            "Epoch 600: Loss_T = -48.25446319580078, Loss_G = -48.55034255981445\n",
            "Epoch 700: Loss_T = -65.39102172851562, Loss_G = -65.84359741210938\n",
            "Epoch 800: Loss_T = -85.71625518798828, Loss_G = -85.0461654663086\n",
            "Epoch 900: Loss_T = -107.12413787841797, Loss_G = -107.12968444824219\n",
            "Epoch 1000: Loss_T = -131.42962646484375, Loss_G = -131.08428955078125\n",
            "Epoch 1100: Loss_T = -154.3215789794922, Loss_G = -155.42184448242188\n",
            "Epoch 1200: Loss_T = -180.03778076171875, Loss_G = -180.69631958007812\n",
            "Epoch 1300: Loss_T = -206.54891967773438, Loss_G = -205.88870239257812\n",
            "Epoch 1400: Loss_T = -233.8482208251953, Loss_G = -234.5735321044922\n",
            "Epoch 1500: Loss_T = -262.4759216308594, Loss_G = -262.72833251953125\n",
            "Epoch 1600: Loss_T = -293.8067932128906, Loss_G = -293.84625244140625\n",
            "Epoch 1700: Loss_T = -325.5715637207031, Loss_G = -327.42730712890625\n",
            "Epoch 1800: Loss_T = -357.6439208984375, Loss_G = -361.61090087890625\n",
            "Epoch 1900: Loss_T = -395.47991943359375, Loss_G = -393.20745849609375\n",
            "Epoch 2000: Loss_T = -427.2517395019531, Loss_G = -429.8265075683594\n",
            "Epoch 2100: Loss_T = -465.0524597167969, Loss_G = -463.108154296875\n",
            "Epoch 2200: Loss_T = -497.0283203125, Loss_G = -498.0159606933594\n",
            "Epoch 2300: Loss_T = -533.517333984375, Loss_G = -533.3472290039062\n",
            "Epoch 2400: Loss_T = -567.9077758789062, Loss_G = -571.681640625\n",
            "Epoch 2500: Loss_T = -607.0667114257812, Loss_G = -605.9133911132812\n",
            "Epoch 2600: Loss_T = -646.0135498046875, Loss_G = -642.2691040039062\n",
            "Epoch 2700: Loss_T = -683.59228515625, Loss_G = -681.126953125\n",
            "Epoch 2800: Loss_T = -721.8641967773438, Loss_G = -718.0127563476562\n",
            "Epoch 2900: Loss_T = -765.6331176757812, Loss_G = -762.1461181640625\n",
            "Epoch 3000: Loss_T = -803.6085815429688, Loss_G = -804.3023071289062\n",
            "Epoch 3100: Loss_T = -842.0657348632812, Loss_G = -844.8796997070312\n",
            "Epoch 3200: Loss_T = -880.7353515625, Loss_G = -880.2506103515625\n",
            "Epoch 3300: Loss_T = -917.5362548828125, Loss_G = -916.912353515625\n",
            "Epoch 3400: Loss_T = -959.75146484375, Loss_G = -959.4844360351562\n",
            "Epoch 3500: Loss_T = -1000.0885009765625, Loss_G = -1001.559326171875\n",
            "Epoch 3600: Loss_T = -1036.95654296875, Loss_G = -1042.180419921875\n",
            "Epoch 3700: Loss_T = -1082.673828125, Loss_G = -1080.320556640625\n",
            "Epoch 3800: Loss_T = -1120.6832275390625, Loss_G = -1124.133544921875\n",
            "Epoch 3900: Loss_T = -1173.5059814453125, Loss_G = -1165.6878662109375\n",
            "Epoch 4000: Loss_T = -1209.01953125, Loss_G = -1211.5091552734375\n",
            "Epoch 4100: Loss_T = -1254.078857421875, Loss_G = -1249.854736328125\n",
            "Epoch 4200: Loss_T = -1294.5382080078125, Loss_G = -1287.630859375\n",
            "Epoch 4300: Loss_T = -1329.677978515625, Loss_G = -1331.7491455078125\n",
            "Epoch 4400: Loss_T = -1368.4100341796875, Loss_G = -1372.3514404296875\n",
            "Epoch 4500: Loss_T = -1415.923095703125, Loss_G = -1414.683349609375\n",
            "Epoch 4600: Loss_T = -1459.5355224609375, Loss_G = -1456.7054443359375\n",
            "Epoch 4700: Loss_T = -1498.744873046875, Loss_G = -1496.63720703125\n",
            "Epoch 4800: Loss_T = -1538.5179443359375, Loss_G = -1541.7891845703125\n",
            "Epoch 4900: Loss_T = -1582.407470703125, Loss_G = -1589.3096923828125\n",
            "Epoch 5000: Loss_T = -1624.3226318359375, Loss_G = -1612.473876953125\n",
            "Epoch 5100: Loss_T = -1662.103515625, Loss_G = -1667.210205078125\n",
            "Epoch 5200: Loss_T = -1706.91552734375, Loss_G = -1701.708740234375\n",
            "Epoch 5300: Loss_T = -1743.003662109375, Loss_G = -1738.11279296875\n",
            "Epoch 5400: Loss_T = -1786.036865234375, Loss_G = -1781.4337158203125\n",
            "Epoch 5500: Loss_T = -1824.1016845703125, Loss_G = -1832.621826171875\n",
            "Epoch 5600: Loss_T = -1867.758056640625, Loss_G = -1867.676025390625\n",
            "Epoch 5700: Loss_T = -1908.56591796875, Loss_G = -1912.22314453125\n",
            "Epoch 5800: Loss_T = -1959.7064208984375, Loss_G = -1953.954345703125\n",
            "Epoch 5900: Loss_T = -1993.839599609375, Loss_G = -1994.3602294921875\n",
            "Epoch 6000: Loss_T = -2041.3802490234375, Loss_G = -2031.2730712890625\n",
            "Epoch 6100: Loss_T = -2077.208251953125, Loss_G = -2066.92236328125\n",
            "Epoch 6200: Loss_T = -2109.043701171875, Loss_G = -2115.84033203125\n",
            "Epoch 6300: Loss_T = -2156.054443359375, Loss_G = -2154.9716796875\n",
            "Epoch 6400: Loss_T = -2192.16455078125, Loss_G = -2195.67626953125\n",
            "Epoch 6500: Loss_T = -2224.39404296875, Loss_G = -2214.215576171875\n",
            "Epoch 6600: Loss_T = -2269.167236328125, Loss_G = -2273.09033203125\n",
            "Epoch 6700: Loss_T = -2316.55712890625, Loss_G = -2298.705810546875\n",
            "Epoch 6800: Loss_T = -2357.994384765625, Loss_G = -2346.9169921875\n",
            "Epoch 6900: Loss_T = -2395.685791015625, Loss_G = -2388.562744140625\n",
            "Epoch 7000: Loss_T = -2439.974609375, Loss_G = -2434.817138671875\n",
            "Epoch 7100: Loss_T = -2476.594482421875, Loss_G = -2461.56787109375\n",
            "Epoch 7200: Loss_T = -2511.37451171875, Loss_G = -2507.232421875\n",
            "Epoch 7300: Loss_T = -2549.2802734375, Loss_G = -2553.9580078125\n",
            "Epoch 7400: Loss_T = -2578.85107421875, Loss_G = -2574.510009765625\n",
            "Epoch 7500: Loss_T = -2620.6494140625, Loss_G = -2630.1123046875\n",
            "Epoch 7600: Loss_T = -2657.18603515625, Loss_G = -2661.68994140625\n",
            "Epoch 7700: Loss_T = -2712.89013671875, Loss_G = -2704.047607421875\n",
            "Epoch 7800: Loss_T = -2745.463623046875, Loss_G = -2739.754638671875\n",
            "Epoch 7900: Loss_T = -2768.112060546875, Loss_G = -2771.181884765625\n",
            "Epoch 8000: Loss_T = -2810.354736328125, Loss_G = -2813.2978515625\n",
            "Epoch 8100: Loss_T = -2864.326171875, Loss_G = -2856.833984375\n",
            "Epoch 8200: Loss_T = -2877.69677734375, Loss_G = -2882.599853515625\n",
            "Epoch 8300: Loss_T = -2935.169921875, Loss_G = -2919.4482421875\n",
            "Epoch 8400: Loss_T = -2956.2236328125, Loss_G = -2975.451904296875\n",
            "Epoch 8500: Loss_T = -2973.50390625, Loss_G = -2987.06689453125\n",
            "Epoch 8600: Loss_T = -3022.068359375, Loss_G = -3046.150634765625\n",
            "Epoch 8700: Loss_T = -3072.5205078125, Loss_G = -3066.311279296875\n",
            "Epoch 8800: Loss_T = -3106.29150390625, Loss_G = -3099.646484375\n",
            "Epoch 8900: Loss_T = -3150.07568359375, Loss_G = -3154.465087890625\n",
            "Epoch 9000: Loss_T = -3189.5654296875, Loss_G = -3165.085205078125\n",
            "Epoch 9100: Loss_T = -3219.166748046875, Loss_G = -3219.67822265625\n",
            "Epoch 9200: Loss_T = -3244.089111328125, Loss_G = -3253.92578125\n",
            "Epoch 9300: Loss_T = -3277.5751953125, Loss_G = -3272.70556640625\n",
            "Epoch 9400: Loss_T = -3326.381103515625, Loss_G = -3318.02880859375\n",
            "Epoch 9500: Loss_T = -3342.64013671875, Loss_G = -3339.473388671875\n",
            "Epoch 9600: Loss_T = -3390.794921875, Loss_G = -3392.68505859375\n",
            "Epoch 9700: Loss_T = -3399.5791015625, Loss_G = -3426.8251953125\n",
            "Epoch 9800: Loss_T = -3462.9248046875, Loss_G = -3454.17724609375\n",
            "Epoch 9900: Loss_T = -3465.24169921875, Loss_G = -3483.680908203125\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.01, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = 0.12492296099662781, Loss_G = 0.041025836020708084\n",
            "Epoch 100: Loss_T = -102.04092407226562, Loss_G = -104.2035903930664\n",
            "Epoch 200: Loss_T = -480.4498291015625, Loss_G = -481.6731872558594\n",
            "Epoch 300: Loss_T = -1148.4473876953125, Loss_G = -1152.9368896484375\n",
            "Epoch 400: Loss_T = -2094.483154296875, Loss_G = -2105.43310546875\n",
            "Epoch 500: Loss_T = -3307.240966796875, Loss_G = -3307.6650390625\n",
            "Epoch 600: Loss_T = -4767.6044921875, Loss_G = -4780.93115234375\n",
            "Epoch 700: Loss_T = -6470.46240234375, Loss_G = -6496.15283203125\n",
            "Epoch 800: Loss_T = -8414.541015625, Loss_G = -8438.607421875\n",
            "Epoch 900: Loss_T = -10563.38671875, Loss_G = -10633.99609375\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.01, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = 0.018216146156191826, Loss_G = -0.08394540101289749\n",
            "Epoch 100: Loss_T = -102.52192687988281, Loss_G = -103.79006958007812\n",
            "Epoch 200: Loss_T = -478.5535888671875, Loss_G = -485.5972900390625\n",
            "Epoch 300: Loss_T = -1145.81591796875, Loss_G = -1160.099853515625\n",
            "Epoch 400: Loss_T = -2089.07763671875, Loss_G = -2099.68310546875\n",
            "Epoch 500: Loss_T = -3313.20458984375, Loss_G = -3314.1455078125\n",
            "Epoch 600: Loss_T = -4751.2353515625, Loss_G = -4765.90234375\n",
            "Epoch 700: Loss_T = -6473.37744140625, Loss_G = -6478.57861328125\n",
            "Epoch 800: Loss_T = -8437.4541015625, Loss_G = -8465.3828125\n",
            "Epoch 900: Loss_T = -10675.345703125, Loss_G = -10632.611328125\n",
            "Epoch 1000: Loss_T = -13024.07421875, Loss_G = -13016.025390625\n",
            "Epoch 1100: Loss_T = -15324.44921875, Loss_G = -15336.466796875\n",
            "Epoch 1200: Loss_T = -17875.8515625, Loss_G = -17761.1484375\n",
            "Epoch 1300: Loss_T = -20465.7109375, Loss_G = -20463.154296875\n",
            "Epoch 1400: Loss_T = -23112.92578125, Loss_G = -23292.197265625\n",
            "Epoch 1500: Loss_T = -26109.75390625, Loss_G = -26125.1640625\n",
            "Epoch 1600: Loss_T = -29271.455078125, Loss_G = -29253.865234375\n",
            "Epoch 1700: Loss_T = -32395.224609375, Loss_G = -32405.02734375\n",
            "Epoch 1800: Loss_T = -35537.1640625, Loss_G = -35709.40625\n",
            "Epoch 1900: Loss_T = -39110.84765625, Loss_G = -38987.67578125\n",
            "Epoch 2000: Loss_T = -42739.2734375, Loss_G = -42578.0703125\n",
            "Epoch 2100: Loss_T = -45913.8671875, Loss_G = -46228.8671875\n",
            "Epoch 2200: Loss_T = -49494.52734375, Loss_G = -49547.828125\n",
            "Epoch 2300: Loss_T = -52884.27734375, Loss_G = -53368.6484375\n",
            "Epoch 2400: Loss_T = -56662.93359375, Loss_G = -56491.453125\n",
            "Epoch 2500: Loss_T = -60533.9140625, Loss_G = -60431.71875\n",
            "Epoch 2600: Loss_T = -64086.65625, Loss_G = -63954.328125\n",
            "Epoch 2700: Loss_T = -67921.3671875, Loss_G = -68032.4375\n",
            "Epoch 2800: Loss_T = -71967.8359375, Loss_G = -71751.109375\n",
            "Epoch 2900: Loss_T = -76012.515625, Loss_G = -76120.5546875\n",
            "Epoch 3000: Loss_T = -80187.84375, Loss_G = -80020.453125\n",
            "Epoch 3100: Loss_T = -83408.4453125, Loss_G = -84161.28125\n",
            "Epoch 3200: Loss_T = -87719.5625, Loss_G = -87842.171875\n",
            "Epoch 3300: Loss_T = -91609.25, Loss_G = -91527.09375\n",
            "Epoch 3400: Loss_T = -95755.1796875, Loss_G = -95524.625\n",
            "Epoch 3500: Loss_T = -99909.4296875, Loss_G = -99714.03125\n",
            "Epoch 3600: Loss_T = -103909.8515625, Loss_G = -102918.421875\n",
            "Epoch 3700: Loss_T = -107189.828125, Loss_G = -107932.15625\n",
            "Epoch 3800: Loss_T = -111772.9296875, Loss_G = -111736.9140625\n",
            "Epoch 3900: Loss_T = -116422.8515625, Loss_G = -115975.8515625\n",
            "Epoch 4000: Loss_T = -120556.125, Loss_G = -120493.6171875\n",
            "Epoch 4100: Loss_T = -124896.71875, Loss_G = -124930.921875\n",
            "Epoch 4200: Loss_T = -128724.03125, Loss_G = -128222.765625\n",
            "Epoch 4300: Loss_T = -132204.0, Loss_G = -132701.875\n",
            "Epoch 4400: Loss_T = -136021.421875, Loss_G = -136584.234375\n",
            "Epoch 4500: Loss_T = -140253.46875, Loss_G = -140743.515625\n",
            "Epoch 4600: Loss_T = -145448.09375, Loss_G = -145066.765625\n",
            "Epoch 4700: Loss_T = -149546.78125, Loss_G = -149059.96875\n",
            "Epoch 4800: Loss_T = -152966.0625, Loss_G = -153572.0\n",
            "Epoch 4900: Loss_T = -157383.28125, Loss_G = -156960.734375\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.01, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = -0.14906080067157745, Loss_G = -0.23588025569915771\n",
            "Epoch 100: Loss_T = -104.13301849365234, Loss_G = -105.90985870361328\n",
            "Epoch 200: Loss_T = -482.998779296875, Loss_G = -486.9131774902344\n",
            "Epoch 300: Loss_T = -1152.1640625, Loss_G = -1154.9644775390625\n",
            "Epoch 400: Loss_T = -2081.110107421875, Loss_G = -2103.8525390625\n",
            "Epoch 500: Loss_T = -3292.661376953125, Loss_G = -3315.05224609375\n",
            "Epoch 600: Loss_T = -4766.046875, Loss_G = -4777.84716796875\n",
            "Epoch 700: Loss_T = -6495.30419921875, Loss_G = -6503.7900390625\n",
            "Epoch 800: Loss_T = -8413.587890625, Loss_G = -8444.1015625\n",
            "Epoch 900: Loss_T = -10561.1796875, Loss_G = -10580.072265625\n",
            "Epoch 1000: Loss_T = -12897.263671875, Loss_G = -12993.40625\n",
            "Epoch 1100: Loss_T = -15366.12890625, Loss_G = -15384.98828125\n",
            "Epoch 1200: Loss_T = -17692.185546875, Loss_G = -17852.810546875\n",
            "Epoch 1300: Loss_T = -20465.23828125, Loss_G = -20393.74609375\n",
            "Epoch 1400: Loss_T = -23225.158203125, Loss_G = -23254.59375\n",
            "Epoch 1500: Loss_T = -26063.47265625, Loss_G = -26087.712890625\n",
            "Epoch 1600: Loss_T = -29298.611328125, Loss_G = -29186.52734375\n",
            "Epoch 1700: Loss_T = -32329.619140625, Loss_G = -32301.259765625\n",
            "Epoch 1800: Loss_T = -35634.81640625, Loss_G = -35756.35546875\n",
            "Epoch 1900: Loss_T = -39119.0, Loss_G = -39277.52734375\n",
            "Epoch 2000: Loss_T = -42461.921875, Loss_G = -42969.33984375\n",
            "Epoch 2100: Loss_T = -46098.12109375, Loss_G = -46077.625\n",
            "Epoch 2200: Loss_T = -49569.45703125, Loss_G = -49722.76953125\n",
            "Epoch 2300: Loss_T = -52872.171875, Loss_G = -53150.05078125\n",
            "Epoch 2400: Loss_T = -56765.71484375, Loss_G = -56432.85546875\n",
            "Epoch 2500: Loss_T = -60410.5546875, Loss_G = -60283.98046875\n",
            "Epoch 2600: Loss_T = -64600.8359375, Loss_G = -64033.15625\n",
            "Epoch 2700: Loss_T = -67918.90625, Loss_G = -67911.046875\n",
            "Epoch 2800: Loss_T = -71712.5546875, Loss_G = -72094.3046875\n",
            "Epoch 2900: Loss_T = -75870.4375, Loss_G = -75789.8359375\n",
            "Epoch 3000: Loss_T = -80455.2890625, Loss_G = -80330.921875\n",
            "Epoch 3100: Loss_T = -83496.1015625, Loss_G = -83580.1171875\n",
            "Epoch 3200: Loss_T = -88107.6171875, Loss_G = -87466.484375\n",
            "Epoch 3300: Loss_T = -91361.875, Loss_G = -91372.4140625\n",
            "Epoch 3400: Loss_T = -95403.765625, Loss_G = -95802.65625\n",
            "Epoch 3500: Loss_T = -98977.5703125, Loss_G = -99306.90625\n",
            "Epoch 3600: Loss_T = -103701.296875, Loss_G = -103243.1171875\n",
            "Epoch 3700: Loss_T = -107532.7890625, Loss_G = -107424.2578125\n",
            "Epoch 3800: Loss_T = -111750.65625, Loss_G = -112122.3359375\n",
            "Epoch 3900: Loss_T = -116231.796875, Loss_G = -116260.1171875\n",
            "Epoch 4000: Loss_T = -120534.53125, Loss_G = -120614.859375\n",
            "Epoch 4100: Loss_T = -124479.09375, Loss_G = -124709.1015625\n",
            "Epoch 4200: Loss_T = -128652.109375, Loss_G = -128743.1875\n",
            "Epoch 4300: Loss_T = -131722.875, Loss_G = -132346.65625\n",
            "Epoch 4400: Loss_T = -136386.515625, Loss_G = -136252.015625\n",
            "Epoch 4500: Loss_T = -140736.921875, Loss_G = -140954.3125\n",
            "Epoch 4600: Loss_T = -145277.53125, Loss_G = -144931.59375\n",
            "Epoch 4700: Loss_T = -149036.03125, Loss_G = -148531.546875\n",
            "Epoch 4800: Loss_T = -153333.78125, Loss_G = -153338.515625\n",
            "Epoch 4900: Loss_T = -157150.40625, Loss_G = -157724.84375\n",
            "Epoch 5000: Loss_T = -162632.46875, Loss_G = -162198.359375\n",
            "Epoch 5100: Loss_T = -166334.34375, Loss_G = -165832.96875\n",
            "Epoch 5200: Loss_T = -169155.984375, Loss_G = -170294.71875\n",
            "Epoch 5300: Loss_T = -174200.703125, Loss_G = -173866.90625\n",
            "Epoch 5400: Loss_T = -177312.296875, Loss_G = -177805.546875\n",
            "Epoch 5500: Loss_T = -182078.703125, Loss_G = -180652.5\n",
            "Epoch 5600: Loss_T = -186403.9375, Loss_G = -186086.46875\n",
            "Epoch 5700: Loss_T = -189651.875, Loss_G = -190488.828125\n",
            "Epoch 5800: Loss_T = -194712.921875, Loss_G = -194634.21875\n",
            "Epoch 5900: Loss_T = -198240.46875, Loss_G = -198967.875\n",
            "Epoch 6000: Loss_T = -202708.546875, Loss_G = -203229.5\n",
            "Epoch 6100: Loss_T = -206866.21875, Loss_G = -206364.4375\n",
            "Epoch 6200: Loss_T = -210889.40625, Loss_G = -209804.28125\n",
            "Epoch 6300: Loss_T = -213394.71875, Loss_G = -214240.140625\n",
            "Epoch 6400: Loss_T = -218789.296875, Loss_G = -217130.53125\n",
            "Epoch 6500: Loss_T = -222113.296875, Loss_G = -222279.34375\n",
            "Epoch 6600: Loss_T = -225756.4375, Loss_G = -226502.4375\n",
            "Epoch 6700: Loss_T = -230355.328125, Loss_G = -229431.515625\n",
            "Epoch 6800: Loss_T = -235154.453125, Loss_G = -235936.984375\n",
            "Epoch 6900: Loss_T = -238075.546875, Loss_G = -238635.59375\n",
            "Epoch 7000: Loss_T = -243185.4375, Loss_G = -242861.375\n",
            "Epoch 7100: Loss_T = -245512.84375, Loss_G = -245333.15625\n",
            "Epoch 7200: Loss_T = -250796.703125, Loss_G = -249236.03125\n",
            "Epoch 7300: Loss_T = -254552.984375, Loss_G = -254687.34375\n",
            "Epoch 7400: Loss_T = -257832.0, Loss_G = -257596.921875\n",
            "Epoch 7500: Loss_T = -261185.5625, Loss_G = -261575.8125\n",
            "Epoch 7600: Loss_T = -264194.96875, Loss_G = -265030.78125\n",
            "Epoch 7700: Loss_T = -268203.8125, Loss_G = -269761.8125\n",
            "Epoch 7800: Loss_T = -272935.375, Loss_G = -272013.28125\n",
            "Epoch 7900: Loss_T = -276122.625, Loss_G = -276831.84375\n",
            "Epoch 8000: Loss_T = -280162.03125, Loss_G = -281473.53125\n",
            "Epoch 8100: Loss_T = -283383.65625, Loss_G = -284066.65625\n",
            "Epoch 8200: Loss_T = -286641.40625, Loss_G = -287634.125\n",
            "Epoch 8300: Loss_T = -292310.8125, Loss_G = -291910.8125\n",
            "Epoch 8400: Loss_T = -295051.1875, Loss_G = -294203.78125\n",
            "Epoch 8500: Loss_T = -299074.65625, Loss_G = -298736.65625\n",
            "Epoch 8600: Loss_T = -302346.46875, Loss_G = -301247.0\n",
            "Epoch 8700: Loss_T = -306224.84375, Loss_G = -306515.0\n",
            "Epoch 8800: Loss_T = -310123.0625, Loss_G = -310776.59375\n",
            "Epoch 8900: Loss_T = -314484.90625, Loss_G = -311660.3125\n",
            "Epoch 9000: Loss_T = -318943.21875, Loss_G = -318022.28125\n",
            "Epoch 9100: Loss_T = -320776.5625, Loss_G = -320384.9375\n",
            "Epoch 9200: Loss_T = -323791.0, Loss_G = -324640.75\n",
            "Epoch 9300: Loss_T = -326556.875, Loss_G = -327197.125\n",
            "Epoch 9400: Loss_T = -330739.875, Loss_G = -330801.34375\n",
            "Epoch 9500: Loss_T = -335106.53125, Loss_G = -334481.8125\n",
            "Epoch 9600: Loss_T = -337827.8125, Loss_G = -336378.03125\n",
            "Epoch 9700: Loss_T = -339181.71875, Loss_G = -341340.46875\n",
            "Epoch 9800: Loss_T = -343885.0, Loss_G = -345970.96875\n",
            "Epoch 9900: Loss_T = -346437.90625, Loss_G = -347100.625\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.1, 'num_epochs': 1000}\n",
            "Epoch 0: Loss_T = -0.10124467313289642, Loss_G = -1.2063895463943481\n",
            "Epoch 100: Loss_T = -9781.232421875, Loss_G = -9993.52734375\n",
            "Epoch 200: Loss_T = -47411.32421875, Loss_G = -48183.8671875\n",
            "Epoch 300: Loss_T = -114472.09375, Loss_G = -115247.53125\n",
            "Epoch 400: Loss_T = -209315.078125, Loss_G = -209808.046875\n",
            "Epoch 500: Loss_T = -329822.4375, Loss_G = -331572.5\n",
            "Epoch 600: Loss_T = -476219.125, Loss_G = -479685.3125\n",
            "Epoch 700: Loss_T = -648080.375, Loss_G = -651661.6875\n",
            "Epoch 800: Loss_T = -843877.4375, Loss_G = -843470.1875\n",
            "Epoch 900: Loss_T = -1059086.125, Loss_G = -1064025.5\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.1, 'num_epochs': 5000}\n",
            "Epoch 0: Loss_T = -0.03100225143134594, Loss_G = -1.1564927101135254\n",
            "Epoch 100: Loss_T = -9805.166015625, Loss_G = -10047.615234375\n",
            "Epoch 200: Loss_T = -47392.96484375, Loss_G = -48060.1015625\n",
            "Epoch 300: Loss_T = -115219.109375, Loss_G = -115236.890625\n",
            "Epoch 400: Loss_T = -208529.875, Loss_G = -210263.609375\n",
            "Epoch 500: Loss_T = -330224.03125, Loss_G = -332206.5625\n",
            "Epoch 600: Loss_T = -476269.28125, Loss_G = -479862.71875\n",
            "Epoch 700: Loss_T = -646393.375, Loss_G = -650899.625\n",
            "Epoch 800: Loss_T = -848119.9375, Loss_G = -844493.8125\n",
            "Epoch 900: Loss_T = -1056394.5, Loss_G = -1065782.0\n",
            "Epoch 1000: Loss_T = -1290877.375, Loss_G = -1303912.5\n",
            "Epoch 1100: Loss_T = -1539374.375, Loss_G = -1531884.75\n",
            "Epoch 1200: Loss_T = -1779572.0, Loss_G = -1783893.625\n",
            "Epoch 1300: Loss_T = -2040771.125, Loss_G = -2051981.125\n",
            "Epoch 1400: Loss_T = -2322791.75, Loss_G = -2325465.5\n",
            "Epoch 1500: Loss_T = -2625278.75, Loss_G = -2601527.75\n",
            "Epoch 1600: Loss_T = -2908198.0, Loss_G = -2923942.25\n",
            "Epoch 1700: Loss_T = -3262451.0, Loss_G = -3247886.5\n",
            "Epoch 1800: Loss_T = -3570121.0, Loss_G = -3593186.25\n",
            "Epoch 1900: Loss_T = -3921080.5, Loss_G = -3921949.25\n",
            "Epoch 2000: Loss_T = -4255224.0, Loss_G = -4304558.0\n",
            "Epoch 2100: Loss_T = -4601612.5, Loss_G = -4636301.5\n",
            "Epoch 2200: Loss_T = -4961500.5, Loss_G = -4964569.5\n",
            "Epoch 2300: Loss_T = -5322369.0, Loss_G = -5306055.0\n",
            "Epoch 2400: Loss_T = -5686030.0, Loss_G = -5667951.5\n",
            "Epoch 2500: Loss_T = -6027301.0, Loss_G = -6035053.0\n",
            "Epoch 2600: Loss_T = -6427505.0, Loss_G = -6396458.0\n",
            "Epoch 2700: Loss_T = -6795979.5, Loss_G = -6804504.0\n",
            "Epoch 2800: Loss_T = -7210883.0, Loss_G = -7192195.5\n",
            "Epoch 2900: Loss_T = -7600193.0, Loss_G = -7575378.0\n",
            "Epoch 3000: Loss_T = -8004575.0, Loss_G = -7996206.5\n",
            "Epoch 3100: Loss_T = -8394757.0, Loss_G = -8378734.5\n",
            "Epoch 3200: Loss_T = -8809401.0, Loss_G = -8786534.0\n",
            "Epoch 3300: Loss_T = -9162947.0, Loss_G = -9158258.0\n",
            "Epoch 3400: Loss_T = -9493949.0, Loss_G = -9542292.0\n",
            "Epoch 3500: Loss_T = -9928228.0, Loss_G = -9932844.0\n",
            "Epoch 3600: Loss_T = -10395148.0, Loss_G = -10299336.0\n",
            "Epoch 3700: Loss_T = -10792558.0, Loss_G = -10799562.0\n",
            "Epoch 3800: Loss_T = -11153174.0, Loss_G = -11185738.0\n",
            "Epoch 3900: Loss_T = -11588642.0, Loss_G = -11665819.0\n",
            "Epoch 4000: Loss_T = -12037246.0, Loss_G = -12056241.0\n",
            "Epoch 4100: Loss_T = -12461900.0, Loss_G = -12477937.0\n",
            "Epoch 4200: Loss_T = -12795818.0, Loss_G = -12856979.0\n",
            "Epoch 4300: Loss_T = -13275100.0, Loss_G = -13243215.0\n",
            "Epoch 4400: Loss_T = -13611606.0, Loss_G = -13698443.0\n",
            "Epoch 4500: Loss_T = -14104759.0, Loss_G = -14106641.0\n",
            "Epoch 4600: Loss_T = -14428540.0, Loss_G = -14375588.0\n",
            "Epoch 4700: Loss_T = -14874697.0, Loss_G = -14865120.0\n",
            "Epoch 4800: Loss_T = -15267566.0, Loss_G = -15350078.0\n",
            "Epoch 4900: Loss_T = -15686969.0, Loss_G = -15782083.0\n",
            "Training with params: {'batch_size': 2048, 'learning_rate': 0.1, 'num_epochs': 10000}\n",
            "Epoch 0: Loss_T = -0.006630841642618179, Loss_G = -1.01004159450531\n",
            "Epoch 100: Loss_T = -9754.36328125, Loss_G = -9986.958984375\n",
            "Epoch 200: Loss_T = -47478.4609375, Loss_G = -48096.4140625\n",
            "Epoch 300: Loss_T = -114122.4375, Loss_G = -114816.640625\n",
            "Epoch 400: Loss_T = -209329.75, Loss_G = -209555.84375\n",
            "Epoch 500: Loss_T = -329894.1875, Loss_G = -331627.875\n",
            "Epoch 600: Loss_T = -476328.15625, Loss_G = -476403.1875\n",
            "Epoch 700: Loss_T = -647804.6875, Loss_G = -650345.4375\n",
            "Epoch 800: Loss_T = -846904.125, Loss_G = -843440.625\n",
            "Epoch 900: Loss_T = -1051493.25, Loss_G = -1063998.875\n",
            "Epoch 1000: Loss_T = -1306789.5, Loss_G = -1304824.875\n",
            "Epoch 1100: Loss_T = -1536544.0, Loss_G = -1535600.5\n",
            "Epoch 1200: Loss_T = -1780488.75, Loss_G = -1788595.625\n",
            "Epoch 1300: Loss_T = -2052643.5, Loss_G = -2050280.375\n",
            "Epoch 1400: Loss_T = -2312402.75, Loss_G = -2322781.0\n",
            "Epoch 1500: Loss_T = -2619608.25, Loss_G = -2611957.25\n",
            "Epoch 1600: Loss_T = -2916625.5, Loss_G = -2919895.25\n",
            "Epoch 1700: Loss_T = -3235558.75, Loss_G = -3234943.25\n",
            "Epoch 1800: Loss_T = -3561950.5, Loss_G = -3573314.0\n",
            "Epoch 1900: Loss_T = -3912018.75, Loss_G = -3909214.0\n",
            "Epoch 2000: Loss_T = -4276333.0, Loss_G = -4295187.0\n",
            "Epoch 2100: Loss_T = -4594587.0, Loss_G = -4628835.5\n",
            "Epoch 2200: Loss_T = -4933650.0, Loss_G = -4953610.0\n",
            "Epoch 2300: Loss_T = -5286291.0, Loss_G = -5297291.0\n",
            "Epoch 2400: Loss_T = -5651677.5, Loss_G = -5674751.0\n",
            "Epoch 2500: Loss_T = -6021483.5, Loss_G = -6042466.5\n",
            "Epoch 2600: Loss_T = -6396833.5, Loss_G = -6392936.5\n",
            "Epoch 2700: Loss_T = -6815404.0, Loss_G = -6803719.0\n",
            "Epoch 2800: Loss_T = -7174971.5, Loss_G = -7196085.5\n",
            "Epoch 2900: Loss_T = -7603479.5, Loss_G = -7588251.5\n",
            "Epoch 3000: Loss_T = -7987808.5, Loss_G = -7999638.5\n",
            "Epoch 3100: Loss_T = -8386329.0, Loss_G = -8379488.5\n",
            "Epoch 3200: Loss_T = -8725804.0, Loss_G = -8749796.0\n",
            "Epoch 3300: Loss_T = -9144524.0, Loss_G = -9157707.0\n",
            "Epoch 3400: Loss_T = -9541030.0, Loss_G = -9561982.0\n",
            "Epoch 3500: Loss_T = -9974622.0, Loss_G = -9924588.0\n",
            "Epoch 3600: Loss_T = -10404652.0, Loss_G = -10407679.0\n",
            "Epoch 3700: Loss_T = -10775596.0, Loss_G = -10762556.0\n",
            "Epoch 3800: Loss_T = -11147469.0, Loss_G = -11171636.0\n",
            "Epoch 3900: Loss_T = -11571335.0, Loss_G = -11612893.0\n",
            "Epoch 4000: Loss_T = -12048343.0, Loss_G = -12088554.0\n",
            "Epoch 4100: Loss_T = -12442427.0, Loss_G = -12414825.0\n",
            "Epoch 4200: Loss_T = -12840311.0, Loss_G = -12855952.0\n",
            "Epoch 4300: Loss_T = -13288450.0, Loss_G = -13281586.0\n",
            "Epoch 4400: Loss_T = -13677153.0, Loss_G = -13695630.0\n",
            "Epoch 4500: Loss_T = -14098555.0, Loss_G = -14098784.0\n",
            "Epoch 4600: Loss_T = -14482702.0, Loss_G = -14526336.0\n",
            "Epoch 4700: Loss_T = -14839481.0, Loss_G = -14940481.0\n",
            "Epoch 4800: Loss_T = -15306481.0, Loss_G = -15290948.0\n",
            "Epoch 4900: Loss_T = -15775312.0, Loss_G = -15788958.0\n",
            "Epoch 5000: Loss_T = -16165774.0, Loss_G = -16171989.0\n",
            "Epoch 5100: Loss_T = -16607148.0, Loss_G = -16587122.0\n",
            "Epoch 5200: Loss_T = -17048554.0, Loss_G = -16967032.0\n",
            "Epoch 5300: Loss_T = -17345684.0, Loss_G = -17491036.0\n",
            "Epoch 5400: Loss_T = -17751196.0, Loss_G = -17741836.0\n",
            "Epoch 5500: Loss_T = -18171146.0, Loss_G = -18317720.0\n",
            "Epoch 5600: Loss_T = -18635976.0, Loss_G = -18491514.0\n",
            "Epoch 5700: Loss_T = -19016366.0, Loss_G = -18937984.0\n",
            "Epoch 5800: Loss_T = -19417332.0, Loss_G = -19486642.0\n",
            "Epoch 5900: Loss_T = -19797114.0, Loss_G = -19887638.0\n",
            "Epoch 6000: Loss_T = -20250718.0, Loss_G = -20315230.0\n",
            "Epoch 6100: Loss_T = -20725730.0, Loss_G = -20697822.0\n",
            "Epoch 6200: Loss_T = -21042720.0, Loss_G = -21050084.0\n",
            "Epoch 6300: Loss_T = -21492828.0, Loss_G = -21530088.0\n",
            "Epoch 6400: Loss_T = -21846190.0, Loss_G = -21828198.0\n",
            "Epoch 6500: Loss_T = -22212890.0, Loss_G = -22297956.0\n",
            "Epoch 6600: Loss_T = -22524644.0, Loss_G = -22774850.0\n",
            "Epoch 6700: Loss_T = -23041522.0, Loss_G = -22972616.0\n",
            "Epoch 6800: Loss_T = -23434924.0, Loss_G = -23472976.0\n",
            "Epoch 6900: Loss_T = -23846118.0, Loss_G = -23928066.0\n",
            "Epoch 7000: Loss_T = -24277270.0, Loss_G = -24303134.0\n",
            "Epoch 7100: Loss_T = -24682946.0, Loss_G = -24554422.0\n",
            "Epoch 7200: Loss_T = -24998246.0, Loss_G = -25061188.0\n",
            "Epoch 7300: Loss_T = -25450326.0, Loss_G = -25317352.0\n",
            "Epoch 7400: Loss_T = -25742432.0, Loss_G = -25852796.0\n",
            "Epoch 7500: Loss_T = -26166818.0, Loss_G = -26211298.0\n",
            "Epoch 7600: Loss_T = -26584948.0, Loss_G = -26455002.0\n",
            "Epoch 7700: Loss_T = -26815356.0, Loss_G = -26874570.0\n",
            "Epoch 7800: Loss_T = -27269096.0, Loss_G = -27218706.0\n",
            "Epoch 7900: Loss_T = -27674332.0, Loss_G = -27652690.0\n",
            "Epoch 8000: Loss_T = -28117996.0, Loss_G = -28163000.0\n",
            "Epoch 8100: Loss_T = -28444078.0, Loss_G = -28443610.0\n",
            "Epoch 8200: Loss_T = -28752328.0, Loss_G = -28827976.0\n",
            "Epoch 8300: Loss_T = -29003166.0, Loss_G = -29261736.0\n",
            "Epoch 8400: Loss_T = -29514310.0, Loss_G = -29532912.0\n",
            "Epoch 8500: Loss_T = -29909662.0, Loss_G = -29731594.0\n",
            "Epoch 8600: Loss_T = -30203980.0, Loss_G = -30439190.0\n",
            "Epoch 8700: Loss_T = -30610696.0, Loss_G = -30694148.0\n",
            "Epoch 8800: Loss_T = -30891830.0, Loss_G = -31084810.0\n",
            "Epoch 8900: Loss_T = -31285820.0, Loss_G = -31332268.0\n",
            "Epoch 9000: Loss_T = -31589560.0, Loss_G = -31770534.0\n",
            "Epoch 9100: Loss_T = -31987786.0, Loss_G = -32089432.0\n",
            "Epoch 9200: Loss_T = -32382032.0, Loss_G = -32236754.0\n",
            "Epoch 9300: Loss_T = -32709408.0, Loss_G = -32728056.0\n",
            "Epoch 9400: Loss_T = -32962760.0, Loss_G = -33045464.0\n",
            "Epoch 9500: Loss_T = -33272988.0, Loss_G = -33358492.0\n",
            "Epoch 9600: Loss_T = -33918264.0, Loss_G = -33815516.0\n",
            "Epoch 9700: Loss_T = -34123160.0, Loss_G = -33997648.0\n",
            "Epoch 9800: Loss_T = -34521180.0, Loss_G = -34377632.0\n",
            "Epoch 9900: Loss_T = -34830456.0, Loss_G = -34898428.0\n",
            "Best Loss: -35433284.0\n",
            "Best Hyperparameters: {'batch_size': 512, 'learning_rate': 0.1, 'num_epochs': 10000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learned_mu = G.mu.item()\n",
        "learned_sigma = torch.exp(G.log_sigma).item()\n",
        "\n",
        "print(\"μ̂ :\", learned_mu)\n",
        "print(\"σ̂ :\", learned_sigma)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K737jMAnlHe3",
        "outputId": "195e2ac1-e0eb-4eaf-de30-2d2d09a86d96"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "μ̂ : -3.534200668334961\n",
            "σ̂ : 0.17280687391757965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mu_optimal= 1.0099984454639233\n",
        "sigma_optimal=1.8304447165750592"
      ],
      "metadata": {
        "id": "WVMraA7xnt1G"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "# Les valeurs pour l'axe des x\n",
        "x_axis = np.linspace(-5, 8, 1000)\n",
        "\n",
        "# La densité de probabilité du mélange de gaussiennes\n",
        "density_mixture = w * norm.pdf(x_axis, m1, np.sqrt(v1)) + (1 - w) * norm.pdf(x_axis, m2, np.sqrt(v2))\n",
        "\n",
        "# Supposons que les densités apprises et la meilleure approximation soient les suivantes :\n",
        "\n",
        "learned_density = norm.pdf(x_axis, learned_mu, learned_sigma)\n",
        "\n",
        "# La densité de la gaussienne optimale\n",
        "optimal_density = norm.pdf(x_axis, mu_optimal, sigma_optimal)\n",
        "\n",
        "\n",
        "# Dessiner la densité de probabilité du mélange de gaussiennes\n",
        "plt.plot(x_axis, density_mixture, label='Mélange de gaussiennes', color='black')\n",
        "\n",
        "# Dessiner la densité apprise\n",
        "plt.plot(x_axis, learned_density, label='Gaussienne générée', linestyle=':', color='red')\n",
        "\n",
        "# Tracer la densité optimale\n",
        "plt.plot(x_axis, optimal_density, label='Meilleure approximation gaussienne', linestyle='--', color='blue')\n",
        "\n",
        "# Ajouter des légendes et des titres\n",
        "plt.title('Comparaison des distributions (divergence KL)')\n",
        "plt.xlabel('Valeur')\n",
        "plt.ylabel('Densité de probabilité')\n",
        "plt.legend()\n",
        "\n",
        "# Afficher le graphique\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "G4wVER7ek2hw",
        "outputId": "54885af8-bf1a-4078-a8ee-f974aa7e80f4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z5VA83pxo8Dv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}